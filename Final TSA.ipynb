{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df=pd.read_csv(\"final_data_in_ML.csv\",parse_dates=['Standardized_Date'])\n",
    "df['DATETIME'] = pd.to_datetime(df['Standardized_Date'].astype(str) + ' ' + df['STANDARDIZED_TIME'].astype(str))\n",
    "df=df[['DATETIME','CLEAR WATER PUMPING FLOW ML','remarks category']]\n",
    "df.set_index('DATETIME', inplace=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_daily = df.resample('D').sum(numeric_only=True)\n",
    "df_daily"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "# 1 month rolling mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the 30-day rolling mean for 'CLEAR WATER PUMPING FLOW ML'\n",
    "df_daily['Rolling_Mean'] = df_daily['CLEAR WATER PUMPING FLOW ML'].rolling(window=30, min_periods=1).mean()\n",
    "df_daily"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace values below 40 with the 30-day rolling mean\n",
    "df_daily['CLEAR WATER PUMPING FLOW ML'] = df_daily.apply(\n",
    "    lambda row: row['Rolling_Mean'] if row['CLEAR WATER PUMPING FLOW ML'] < 40 else row['CLEAR WATER PUMPING FLOW ML'], axis=1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_daily.loc['2024-08']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_daily.loc['2022-07']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "2022-07-04\t33.713360\n",
    "2022-07-06\t36.777820\n",
    "2022-07-07\t7.301890\n",
    "2022-07-08\t28.993830\n",
    "2022-07-14\t39.644030\n",
    "2022-07-15\t37.129840\n",
    "2022-07-16\t32.285300\n",
    "2022-07-18\t33.522280\n",
    "2022-07-19\t37.071320\n",
    "2022-07-20\t30.800720\n",
    "2022-07-21\t22.001960\n",
    "2022-07-27\t39.910050"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pmdarima import auto_arima\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "from statsmodels.tsa.arima.model import ARIMA,ARIMAResults\n",
    "from sklearn.metrics import mean_squared_error,mean_absolute_percentage_error\n",
    "from statsmodels.tools.eval_measures import rmse\n",
    "\n",
    "class TSA:\n",
    "    def __init__(self,df):\n",
    "        self.df = df\n",
    "        \n",
    "    def adf_test(self,valcol):\n",
    "        \"\"\"\n",
    "        Pass in a time series and an optional title, returns an ADF report\n",
    "        \"\"\"\n",
    "        result = adfuller(self.df[valcol].dropna(),autolag='AIC') # .dropna() handles differenced data\n",
    "    \n",
    "        labels = ['ADF test statistic','p-value','# lags used','# observations']\n",
    "        out = pd.Series(result[0:4],index=labels)\n",
    "\n",
    "        for key,val in result[4].items():\n",
    "            out[f'critical value ({key})']=val\n",
    "        \n",
    "        print(out.to_string())          # .to_string() removes the line \"dtype: float64\"\n",
    "    \n",
    "        if result[1] <= 0.05:\n",
    "            print(\"Strong evidence against the null hypothesis\")\n",
    "            print(\"Reject the null hypothesis\")\n",
    "            print(\"Data has no unit root and is stationary\")\n",
    "            state = \"Stationary\"\n",
    "        else:\n",
    "            print(\"Weak evidence against the null hypothesis\")\n",
    "            print(\"Fail to reject the null hypothesis\")\n",
    "            print(\"Data has a unit root and is non-stationary\")\n",
    "            state = \"Non-stationary\"\n",
    "        if state == \"Stationary\":\n",
    "            return \"Yes\"\n",
    "        else:\n",
    "            return \"No\"\n",
    "    def determine_ARIMA_order(self,valcol):\n",
    "        stepwise_fit = auto_arima(self.df[valcol], start_p=0, start_q=0,\n",
    "                          error_action='ignore',   # we don't want to know if an order does not work\n",
    "                          suppress_warnings=True,  # we don't want convergence warnings\n",
    "                          stepwise=True)           # set to stepwise\n",
    "        best_order = stepwise_fit.get_params().get('order')\n",
    "        print('The best order is {}'.format(best_order))\n",
    "        return best_order\n",
    "    def fit_model(self,valcol):\n",
    "#         if len(self.df[valcol]) > 70:\n",
    "        train = self.df[valcol][:len(self.df[valcol])-60]#30 test + 30 val =60 train (means except last 60 rows all others taken as train)\n",
    "        test = self.df[valcol][len(self.df[valcol])-60:len(self.df[valcol])-30] #(60-30=30)second last 30 rows \n",
    "        val = self.df[valcol][len(self.df[valcol])-30:] #last 30 rows\n",
    "#         else:\n",
    "#             train = self.df[valcol][:len(self.df[valcol])-8]\n",
    "#             test = self.df[valcol][len(self.df[valcol])-8:len(self.df[valcol])-4]\n",
    "#             val = self.df[valcol][len(self.df[valcol])-4:]\n",
    "        start = len(train)\n",
    "        end = len(train)+len(test)-1\n",
    "#             print('train : {}'.format(train))\n",
    "#             print('test : {}'.format(test))\n",
    "        print('start : {}'.format(start))\n",
    "        print('end : {}'.format(end))\n",
    "        results = ARIMA(train,order=c1.determine_ARIMA_order(valcol)).fit()\n",
    "        predictions = results.predict(start=start, end=end)\n",
    "        predictions_val = results.predict(start=end+1, end=len(train)+len(test)+len(val)-1)\n",
    "        error1 = mean_squared_error(test, predictions)\n",
    "        error2 = rmse(test, predictions)\n",
    "        error3 = mean_absolute_percentage_error(test,predictions)\n",
    "        accuracy = (1-error3)*100\n",
    "        print(f'MSE Error: {error1:11.10}')\n",
    "        print(f'RMSE Error: {error2:11.10}')\n",
    "        print(f'MAPE Error: {error3:11.10}')\n",
    "        print(f'Accuracy: {accuracy:11.10}')\n",
    "#         return start, len(test)\n",
    "#         model_results = {\"Model\":\"ARIMA\",\"Stationary\": c1.adf_test(valcol),\"X_train\": str(len(train))+\" Weeks\", \"X_test\": str(len(test))+\" Weeks\", \"X_validation\": str(len(val))+\" Weeks\", \"ARIMA_order\": c1.determine_ARIMA_order(valcol), \"MSE\": error1,\"RMSE\": error2,\"MAPE\":error3, \"Accuracy\":((1-error3)*100).round(0)}\n",
    "#         with open('TSA_AQI_model_result_new.json','a') as f:\n",
    "#             f.write(str(model_results)+',')\n",
    "#             f.close()\n",
    "        return predictions_val\n",
    "        \n",
    "            \n",
    "    def full_data_model(self,valcol):\n",
    "        results = ARIMA(self.df[valcol],order=c1.determine_ARIMA_order(valcol)).fit()\n",
    "#         if len(self.df[valcol]) > 70:\n",
    "        fcast = results.predict(len(self.df), len(self.df)+3,typ='levels').round(2)\n",
    "#         else:\n",
    "#             fcast = results.predict(len(self.df), len(self.df)+3,typ='levels').round(2)\n",
    "#         ax = self.df[valcol].plot(legend=True,figsize=(12,6))\n",
    "#         fcast.plot(legend=True)\n",
    "        print(fcast)\n",
    "#         error_rmse = rmse(self.df[valcol],fcast[0:len(self.df)])\n",
    "#         error_mse = mean_squared_error(self.df[valcol],fcast[0:len(self.df)])\n",
    "        DF = pd.DataFrame(self.df[valcol])\n",
    "        DF['Type'] = 'Actual'\n",
    "#         DF = DF.reset_index()\n",
    "        DF_fcast = pd.DataFrame(fcast)\n",
    "        DF_fcast['Type'] = 'Predicted'\n",
    "        DF_fcast = DF_fcast.rename(columns={'predicted_mean':valcol})\n",
    "        final_DF = pd.concat([DF,DF_fcast])\n",
    "        final_DF = final_DF.reset_index()\n",
    "#         DF_fcast = DF_fcast.rename(columns={'predicted_mean':'Predicted', 'index':'DateTime'})\n",
    "        DF_val = pd.DataFrame(c1.fit_model(valcol))\n",
    "        DF_val = DF_val.reset_index()\n",
    "        DF_val = DF_val.rename(columns={'index':'Date','predicted_mean':'Validation'})\n",
    "        final_DF = final_DF.rename(columns={'index':'Date'})\n",
    "        print(DF_val)\n",
    "        print(final_DF)\n",
    "#         final_DF =  final_DF.merge(DF_val, on='DateTime',how='outer')\n",
    "        final_DF =  final_DF.merge(DF_val, on='Date',how='outer')\n",
    "        final_DF['Date'] = final_DF['Date'].astype('str')\n",
    "        print(final_DF)\n",
    "        \n",
    "#         final_DF.to_json('/Users/syaminiv/Library/CloudStorage/OneDrive-BayesianWaysLLP/Documents/SiAP_ML_Application/Final_Notebooks/June2023/JSON/TSA_withval_Zone_{}.json'.format(item),orient='records')\n",
    "\n",
    "c1 = TSA(df_daily)\n",
    "c1.adf_test(\"CLEAR WATER PUMPING FLOW ML\")\n",
    "c1.determine_ARIMA_order(\"CLEAR WATER PUMPING FLOW ML\")\n",
    "c1.fit_model(\"CLEAR WATER PUMPING FLOW ML\")\n",
    "c1.full_data_model(\"CLEAR WATER PUMPING FLOW ML\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "# 1 month mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the year and month for grouping\n",
    "df_daily['YearMonth'] = df_daily.index.to_period('M')\n",
    "df_daily"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate monthly means only for 'CLEAR WATER PUMPING FLOW ML'\n",
    "monthly_means = df_daily.groupby('YearMonth')['CLEAR WATER PUMPING FLOW ML'].transform('mean')\n",
    "monthly_means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_daily['CLEAR WATER PUMPING FLOW ML'] = df_daily.apply(\n",
    "    lambda row: monthly_means[row.name] if row['CLEAR WATER PUMPING FLOW ML'] < 40 else row['CLEAR WATER PUMPING FLOW ML'], axis=1\n",
    ")\n",
    "df_daily"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_daily.loc['2022-07']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "2022-07-04\t33.713360\n",
    "2022-07-06\t36.777820\n",
    "2022-07-07\t7.301890\n",
    "2022-07-08\t28.993830\n",
    "2022-07-14\t39.644030\n",
    "2022-07-15\t37.129840\n",
    "2022-07-16\t32.285300\n",
    "2022-07-18\t33.522280\n",
    "2022-07-19\t37.071320\n",
    "2022-07-20\t30.800720\n",
    "2022-07-21\t22.001960\n",
    "2022-07-27\t39.910050"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_daily[df_daily['CLEAR WATER PUMPING FLOW ML']<40]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mean_value = df_daily['CLEAR WATER PUMPING FLOW ML'].mean()\n",
    "# # Replace values less than 40 in 'CLEAR WATER PUMPING FLOW ML' with the calculated mean\n",
    "# df_daily['CLEAR WATER PUMPING FLOW ML'] = df_daily['CLEAR WATER PUMPING FLOW ML'].apply(lambda x: mean_value if x < 40 else x)\n",
    "# df_daily"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pmdarima import auto_arima\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "from statsmodels.tsa.arima.model import ARIMA,ARIMAResults\n",
    "from sklearn.metrics import mean_squared_error,mean_absolute_percentage_error\n",
    "from statsmodels.tools.eval_measures import rmse\n",
    "\n",
    "class TSA:\n",
    "    def __init__(self,df):\n",
    "        self.df = df\n",
    "        \n",
    "    def adf_test(self,valcol):\n",
    "        \"\"\"\n",
    "        Pass in a time series and an optional title, returns an ADF report\n",
    "        \"\"\"\n",
    "        result = adfuller(self.df[valcol].dropna(),autolag='AIC') # .dropna() handles differenced data\n",
    "    \n",
    "        labels = ['ADF test statistic','p-value','# lags used','# observations']\n",
    "        out = pd.Series(result[0:4],index=labels)\n",
    "\n",
    "        for key,val in result[4].items():\n",
    "            out[f'critical value ({key})']=val\n",
    "        \n",
    "        print(out.to_string())          # .to_string() removes the line \"dtype: float64\"\n",
    "    \n",
    "        if result[1] <= 0.05:\n",
    "            print(\"Strong evidence against the null hypothesis\")\n",
    "            print(\"Reject the null hypothesis\")\n",
    "            print(\"Data has no unit root and is stationary\")\n",
    "            state = \"Stationary\"\n",
    "        else:\n",
    "            print(\"Weak evidence against the null hypothesis\")\n",
    "            print(\"Fail to reject the null hypothesis\")\n",
    "            print(\"Data has a unit root and is non-stationary\")\n",
    "            state = \"Non-stationary\"\n",
    "        if state == \"Stationary\":\n",
    "            return \"Yes\"\n",
    "        else:\n",
    "            return \"No\"\n",
    "    def determine_ARIMA_order(self,valcol):\n",
    "        stepwise_fit = auto_arima(self.df[valcol], start_p=0, start_q=0,\n",
    "                          error_action='ignore',   # we don't want to know if an order does not work\n",
    "                          suppress_warnings=True,  # we don't want convergence warnings\n",
    "                          stepwise=True)           # set to stepwise\n",
    "        best_order = stepwise_fit.get_params().get('order')\n",
    "        print('The best order is {}'.format(best_order))\n",
    "        return best_order\n",
    "    def fit_model(self,valcol):\n",
    "#         if len(self.df[valcol]) > 70:\n",
    "        train = self.df[valcol][:len(self.df[valcol])-60]#30 test + 30 val =60 train (means except last 60 rows all others taken as train)\n",
    "        test = self.df[valcol][len(self.df[valcol])-60:len(self.df[valcol])-30] #(60-30=30)second last 30 rows \n",
    "        val = self.df[valcol][len(self.df[valcol])-30:] #last 30 rows\n",
    "#         else:\n",
    "#             train = self.df[valcol][:len(self.df[valcol])-8]\n",
    "#             test = self.df[valcol][len(self.df[valcol])-8:len(self.df[valcol])-4]\n",
    "#             val = self.df[valcol][len(self.df[valcol])-4:]\n",
    "        start = len(train)\n",
    "        end = len(train)+len(test)-1\n",
    "#             print('train : {}'.format(train))\n",
    "#             print('test : {}'.format(test))\n",
    "        print('start : {}'.format(start))\n",
    "        print('end : {}'.format(end))\n",
    "        results = ARIMA(train,order=c1.determine_ARIMA_order(valcol)).fit()\n",
    "        predictions = results.predict(start=start, end=end)\n",
    "        predictions_val = results.predict(start=end+1, end=len(train)+len(test)+len(val)-1)\n",
    "        error1 = mean_squared_error(test, predictions)\n",
    "        error2 = rmse(test, predictions)\n",
    "        error3 = mean_absolute_percentage_error(test,predictions)\n",
    "        accuracy = (1-error3)*100\n",
    "        print(f'MSE Error: {error1:11.10}')\n",
    "        print(f'RMSE Error: {error2:11.10}')\n",
    "        print(f'MAPE Error: {error3:11.10}')\n",
    "        print(f'Accuracy: {accuracy:11.10}')\n",
    "#         return start, len(test)\n",
    "#         model_results = {\"Model\":\"ARIMA\",\"Stationary\": c1.adf_test(valcol),\"X_train\": str(len(train))+\" Weeks\", \"X_test\": str(len(test))+\" Weeks\", \"X_validation\": str(len(val))+\" Weeks\", \"ARIMA_order\": c1.determine_ARIMA_order(valcol), \"MSE\": error1,\"RMSE\": error2,\"MAPE\":error3, \"Accuracy\":((1-error3)*100).round(0)}\n",
    "#         with open('TSA_AQI_model_result_new.json','a') as f:\n",
    "#             f.write(str(model_results)+',')\n",
    "#             f.close()\n",
    "        return predictions_val\n",
    "        \n",
    "            \n",
    "    def full_data_model(self,valcol):\n",
    "        results = ARIMA(self.df[valcol],order=c1.determine_ARIMA_order(valcol)).fit()\n",
    "#         if len(self.df[valcol]) > 70:\n",
    "        fcast = results.predict(len(self.df), len(self.df)+3,typ='levels').round(2)\n",
    "#         else:\n",
    "#             fcast = results.predict(len(self.df), len(self.df)+3,typ='levels').round(2)\n",
    "#         ax = self.df[valcol].plot(legend=True,figsize=(12,6))\n",
    "#         fcast.plot(legend=True)\n",
    "        print(fcast)\n",
    "#         error_rmse = rmse(self.df[valcol],fcast[0:len(self.df)])\n",
    "#         error_mse = mean_squared_error(self.df[valcol],fcast[0:len(self.df)])\n",
    "        DF = pd.DataFrame(self.df[valcol])\n",
    "        DF['Type'] = 'Actual'\n",
    "#         DF = DF.reset_index()\n",
    "        DF_fcast = pd.DataFrame(fcast)\n",
    "        DF_fcast['Type'] = 'Predicted'\n",
    "        DF_fcast = DF_fcast.rename(columns={'predicted_mean':valcol})\n",
    "        final_DF = pd.concat([DF,DF_fcast])\n",
    "        final_DF = final_DF.reset_index()\n",
    "#         DF_fcast = DF_fcast.rename(columns={'predicted_mean':'Predicted', 'index':'DateTime'})\n",
    "        DF_val = pd.DataFrame(c1.fit_model(valcol))\n",
    "        DF_val = DF_val.reset_index()\n",
    "        DF_val = DF_val.rename(columns={'index':'Date','predicted_mean':'Validation'})\n",
    "        final_DF = final_DF.rename(columns={'index':'Date'})\n",
    "        print(DF_val)\n",
    "        print(final_DF)\n",
    "#         final_DF =  final_DF.merge(DF_val, on='DateTime',how='outer')\n",
    "        final_DF =  final_DF.merge(DF_val, on='Date',how='outer')\n",
    "        final_DF['Date'] = final_DF['Date'].astype('str')\n",
    "        print(final_DF)\n",
    "        \n",
    "#         final_DF.to_json('/Users/syaminiv/Library/CloudStorage/OneDrive-BayesianWaysLLP/Documents/SiAP_ML_Application/Final_Notebooks/June2023/JSON/TSA_withval_Zone_{}.json'.format(item),orient='records')\n",
    "\n",
    "c1 = TSA(df_daily)\n",
    "c1.adf_test(\"CLEAR WATER PUMPING FLOW ML\")\n",
    "c1.determine_ARIMA_order(\"CLEAR WATER PUMPING FLOW ML\")\n",
    "c1.fit_model(\"CLEAR WATER PUMPING FLOW ML\")\n",
    "c1.full_data_model(\"CLEAR WATER PUMPING FLOW ML\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_percentage_error\n",
    "from statsmodels.tools.eval_measures import rmse\n",
    "import pmdarima as pm\n",
    "import seaborn as sns\n",
    "\n",
    "def forecast_with_arima(df, column, freq, forecast_periods,m):\n",
    "    # Resample the data\n",
    "    df_resampled = df.resample(freq).sum(numeric_only=True)\n",
    "    \n",
    "    # Split data into training and testing\n",
    "    train = df_resampled[column][:-forecast_periods]\n",
    "    test = df_resampled[column][-forecast_periods:]\n",
    "\n",
    "    # Train the ARIMA model using auto_arima\n",
    "    model = pm.auto_arima(df_resampled[column],m=m, seasonal=True, start_p=0, start_q=0, max_order=4, test='adf', trace=True,\n",
    "                          error_action='ignore', suppress_warnings=True, stepwise=True)\n",
    "\n",
    "    # Fit the model and make predictions\n",
    "    results = model.fit(train)\n",
    "    predictions = results.predict(n_periods=forecast_periods)\n",
    "\n",
    "    # Calculate performance metrics\n",
    "    mse_error = mean_squared_error(test, predictions)\n",
    "    rmse_error = rmse(test, predictions)\n",
    "    mape_error = mean_absolute_percentage_error(test, predictions)\n",
    "    accuracy = (1 - mape_error) * 100\n",
    "    \n",
    "    # Print the results\n",
    "    print(f'Frequency: {freq}')\n",
    "    print(f'MSE Error: {mse_error:11.10}')\n",
    "    print(f'RMSE Error: {rmse_error:11.10}')\n",
    "    print(f'MAPE Error: {mape_error:11.10}')\n",
    "    print(f'Accuracy: {accuracy:11.10}')\n",
    "    \n",
    "    # Re-fit the model on the full data and forecast future values\n",
    "    results_full = model.fit(df_resampled[column])\n",
    "    forecast = results_full.predict(n_periods=forecast_periods)\n",
    "    \n",
    "    # Combine actual and predicted data\n",
    "    DF_actual = pd.DataFrame(df_resampled[column])\n",
    "    DF_actual['Type'] = 'Actual'\n",
    "    DF_forecast = pd.DataFrame(forecast, columns=[column])\n",
    "    DF_forecast['Type'] = 'Predicted'\n",
    "    \n",
    "    final_df = pd.concat([DF_actual, DF_forecast])\n",
    "    final_df = final_df.reset_index()\n",
    "    final_df = final_df.rename(columns={'index': 'Date'})\n",
    "    \n",
    "    return final_df\n",
    "\n",
    "# Example of usage\n",
    "df=pd.read_csv(\"final_data_in_ML.csv\",parse_dates=['Standardized_Date'])\n",
    "df['DATETIME'] = pd.to_datetime(df['Standardized_Date'].astype(str) + ' ' + df['STANDARDIZED_TIME'].astype(str))\n",
    "df=df[['DATETIME','CLEAR WATER PUMPING FLOW ML']]\n",
    "df.set_index('DATETIME', inplace=True)\n",
    "\n",
    "# Forecast for daily, weekly, and monthly\n",
    "print(\"Daily_resampled_data\")\n",
    "daily_results = forecast_with_arima(df, 'CLEAR WATER PUMPING FLOW ML', 'D',4,52)\n",
    "print(\"Weekly_resampled_data\")\n",
    "weekly_results = forecast_with_arima(df, 'CLEAR WATER PUMPING FLOW ML', 'W',4,7)\n",
    "print(\"Monthly_resampled_data\")\n",
    "monthly_results = forecast_with_arima(df, 'CLEAR WATER PUMPING FLOW ML', 'M',4,12)\n",
    "\n",
    "# Show the results\n",
    "print(\"Daily_resampled_data\")\n",
    "print(daily_results.tail(60))\n",
    "print(\"Weekly_resampled_data\")\n",
    "print(weekly_results.tail(60))\n",
    "print(\"Monthly_resampled_data\")\n",
    "print(monthly_results.tail(60))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_percentage_error\n",
    "from statsmodels.tools.eval_measures import rmse\n",
    "import pmdarima as pm\n",
    "import seaborn as sns\n",
    "\n",
    "def forecast_with_arima(df, column, freq, forecast_periods, m, threshold=40):\n",
    "    # Resample the data\n",
    "    df_resampled = df.resample(freq).sum(numeric_only=True)\n",
    "    \n",
    "    # Apply threshold logic only for daily frequency\n",
    "    if freq == 'D':\n",
    "        # Calculate the mean of the column\n",
    "        df_resampled['Rolling_Mean'] = df_resampled[column].rolling(window=30, min_periods=1).mean()\n",
    "        df_resampled[column] = df_resampled.apply(lambda row: row['Rolling_Mean'] if row[column] < threshold else row[column], axis=1)\n",
    "\n",
    "#         mean_value = df_resampled[column].mean()\n",
    "#         # Replace values below the threshold with the mean\n",
    "#         df_resampled[column] = df_resampled[column].apply(lambda x: mean_value if x < threshold else x)\n",
    "    \n",
    "    # Split data into training and testing\n",
    "    train = df_resampled[column][:-forecast_periods]\n",
    "    test = df_resampled[column][-forecast_periods:]\n",
    "\n",
    "    # Train the ARIMA model using auto_arima\n",
    "    model = pm.auto_arima(df_resampled[column], m=m, seasonal=True, start_p=0, start_q=0, max_order=4, test='adf', trace=True,\n",
    "                          error_action='ignore', suppress_warnings=True, stepwise=True)\n",
    "\n",
    "    # Fit the model and make predictions\n",
    "    results = model.fit(train)\n",
    "    predictions = results.predict(n_periods=forecast_periods)\n",
    "\n",
    "    # Calculate performance metrics\n",
    "    mse_error = mean_squared_error(test, predictions)\n",
    "    rmse_error = rmse(test, predictions)\n",
    "    mape_error = mean_absolute_percentage_error(test, predictions)\n",
    "    accuracy = (1 - mape_error) * 100\n",
    "    \n",
    "    # Print the results\n",
    "    print(f'Frequency: {freq}')\n",
    "    print(f'MSE Error: {mse_error:11.10}')\n",
    "    print(f'RMSE Error: {rmse_error:11.10}')\n",
    "    print(f'MAPE Error: {mape_error:11.10}')\n",
    "    print(f'Accuracy: {accuracy:11.10}')\n",
    "    \n",
    "    # Re-fit the model on the full data and forecast future values\n",
    "    results_full = model.fit(df_resampled[column])\n",
    "    forecast = results_full.predict(n_periods=forecast_periods)\n",
    "    \n",
    "    # Combine actual and predicted data\n",
    "    DF_actual = pd.DataFrame(df_resampled[column])\n",
    "    DF_actual['Type'] = 'Actual'\n",
    "    DF_forecast = pd.DataFrame(forecast, columns=[column])\n",
    "    DF_forecast['Type'] = 'Predicted'\n",
    "    \n",
    "    final_df = pd.concat([DF_actual, DF_forecast])\n",
    "    final_df = final_df.reset_index()\n",
    "    final_df = final_df.rename(columns={'index': 'Date'})\n",
    "    \n",
    "    return final_df\n",
    "\n",
    "# Example of usage\n",
    "df = pd.read_csv(\"final_data_in_ML.csv\", parse_dates=['Standardized_Date'])\n",
    "df['DATETIME'] = pd.to_datetime(df['Standardized_Date'].astype(str) + ' ' + df['STANDARDIZED_TIME'].astype(str))\n",
    "df = df[['DATETIME', 'CLEAR WATER PUMPING FLOW ML']]\n",
    "df.set_index('DATETIME', inplace=True)\n",
    "\n",
    "# Forecast for daily, weekly, and monthly\n",
    "print(\"Daily_resampled_data\")\n",
    "daily_results = forecast_with_arima(df, 'CLEAR WATER PUMPING FLOW ML', 'D', 4, 52)\n",
    "print(\"Weekly_resampled_data\")\n",
    "weekly_results = forecast_with_arima(df, 'CLEAR WATER PUMPING FLOW ML', 'W', 4, 7)\n",
    "print(\"Monthly_resampled_data\")\n",
    "monthly_results = forecast_with_arima(df, 'CLEAR WATER PUMPING FLOW ML', 'M', 4, 12)\n",
    "\n",
    "# Show the results\n",
    "print(\"Daily_resampled_data\")\n",
    "print(daily_results.tail(60))\n",
    "print(\"Weekly_resampled_data\")\n",
    "print(weekly_results.tail(60))\n",
    "print(\"Monthly_resampled_data\")\n",
    "print(monthly_results.tail(60))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
