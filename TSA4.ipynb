{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import statsmodels.api as sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the data\n",
    "df = pd.read_csv(\"final_data_in_ML.csv\", parse_dates=['Standardized_Date'])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine 'Standardized_Date' and 'STANDARDIZED_TIME' into a 'DATETIME' column\n",
    "df['DATETIME'] = pd.to_datetime(df['Standardized_Date'].astype(str) + ' ' + df['STANDARDIZED_TIME'].astype(str))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter necessary columns\n",
    "df = df[['DATETIME', 'CLEAR WATER PUMPING FLOW ML', 'remarks category']]\n",
    "df.set_index('DATETIME', inplace=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess the 'REMARKS' column using One-Hot Encoding\n",
    "encoder = OneHotEncoder(sparse_output=False, drop='first')  # drop='first' to avoid multicollinearity\n",
    "remarks_encoded = encoder.fit_transform(df[['remarks category']])\n",
    "\n",
    "# Create a DataFrame for the encoded remarks\n",
    "remarks_encoded_df = pd.DataFrame(remarks_encoded, index=df.index, columns=encoder.get_feature_names_out(['remarks category']))\n",
    "remarks_encoded_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate the encoded remarks back to the original dataframe\n",
    "df = pd.concat([df['CLEAR WATER PUMPING FLOW ML'], remarks_encoded_df], axis=1)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_daily = df.resample('D').sum(numeric_only=True)\n",
    "df_daily"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.statespace.varmax import VARMAX\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "from pmdarima.arima import auto_arima\n",
    "from statsmodels.tsa.arima.model import ARIMA,ARIMAResults\n",
    "from sklearn.metrics import mean_squared_error,mean_absolute_percentage_error\n",
    "from statsmodels.tools.eval_measures import rmse\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "import statsmodels.api as sm\n",
    "\n",
    "class TSA_Multi:\n",
    "    def __init__(self, df):\n",
    "        self.df = df\n",
    "\n",
    "    def adf_test(self, col_list):\n",
    "        for col in col_list:\n",
    "            print(f\"ADF Test for {col}\")\n",
    "            result = adfuller(self.df[col].dropna(), autolag='AIC')\n",
    "            labels = ['ADF test statistic', 'p-value', '# lags used', '# observations']\n",
    "            out = pd.Series(result[0:4], index=labels)\n",
    "\n",
    "            for key, val in result[4].items():\n",
    "                out[f'critical value ({key})'] = val\n",
    "\n",
    "            print(out.to_string())\n",
    "\n",
    "            if result[1] <= 0.05:\n",
    "                print(\"Strong evidence against the null hypothesis\")\n",
    "                print(\"Reject the null hypothesis\")\n",
    "                print(\"Data has no unit root and is stationary\")\n",
    "            else:\n",
    "                print(\"Weak evidence against the null hypothesis\")\n",
    "                print(\"Fail to reject the null hypothesis\")\n",
    "                print(\"Data has a unit root and is non-stationary\")\n",
    "            print()\n",
    "\n",
    "    def fit_model(self, valcol_list):\n",
    "        train = self.df.iloc[:-6, :]\n",
    "        test = self.df.iloc[-6:, :]\n",
    "\n",
    "        print(f\"Training data: {train.shape}, Test data: {test.shape}\")\n",
    "\n",
    "        # Fit a VARMAX model\n",
    "        model = VARMAX(train[valcol_list], order=(2, 1))  # Adjust (p,q) as needed\n",
    "        fitted_model = model.fit(disp=False)\n",
    "\n",
    "        # Forecast\n",
    "        forecast = fitted_model.forecast(steps=len(test))\n",
    "        print(f\"Forecast results:\\n{forecast}\")\n",
    "\n",
    "        # Evaluate model\n",
    "        for col in valcol_list:\n",
    "            mse = mean_squared_error(test[col], forecast[col])\n",
    "            rmse_error = rmse(test[col], forecast[col])\n",
    "            mape = mean_absolute_percentage_error(test[col], forecast[col])\n",
    "            accuracy = (1 - mape) * 100\n",
    "\n",
    "            print(f\"\\nMetrics for {col}:\")\n",
    "            print(f'MSE: {mse}')\n",
    "            print(f'RMSE: {rmse_error}')\n",
    "            print(f'MAPE: {mape}')\n",
    "            print(f'Accuracy: {accuracy}%')\n",
    "\n",
    "        return forecast\n",
    "\n",
    "    def full_data_model(self, valcol_list):\n",
    "        model = VARMAX(self.df[valcol_list], order=(2, 1))  # Adjust (p,q) as needed\n",
    "        fitted_model = model.fit(disp=False)\n",
    "\n",
    "        # Forecast the next 3 periods\n",
    "        forecast = fitted_model.forecast(steps=3)\n",
    "        print(f\"Full Data Forecast results:\\n{forecast}\")\n",
    "\n",
    "        # Create final DataFrame\n",
    "        final_DF = pd.concat([self.df[valcol_list], forecast], axis=0)\n",
    "        final_DF['Type'] = ['Actual'] * len(self.df) + ['Predicted'] * len(forecast)\n",
    "        \n",
    "        print(final_DF.tail(10))\n",
    "        return final_DF\n",
    "\n",
    "# Initialize with the DataFrame containing both CLEAR WATER PUMPING FLOW ML and encoded remarks columns\n",
    "c2 = TSA_Multi(df_daily)\n",
    "\n",
    "# Perform ADF test for all columns (Clear Water and remarks)\n",
    "columns_to_forecast = ['CLEAR WATER PUMPING FLOW ML'] + list(remarks_encoded_df.columns)\n",
    "c2.adf_test(columns_to_forecast)\n",
    "\n",
    "# Fit and forecast the model\n",
    "forecast_results = c2.fit_model(columns_to_forecast)\n",
    "\n",
    "# Forecast for the full dataset and future periods\n",
    "c2.full_data_model(columns_to_forecast)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_monthly = df.resample('M').sum(numeric_only=True)\n",
    "df_monthly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.statespace.varmax import VARMAX\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "from pmdarima.arima import auto_arima\n",
    "from statsmodels.tsa.arima.model import ARIMA,ARIMAResults\n",
    "from sklearn.metrics import mean_squared_error,mean_absolute_percentage_error\n",
    "from statsmodels.tools.eval_measures import rmse\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "import statsmodels.api as sm\n",
    "\n",
    "class TSA_Multi:\n",
    "    def __init__(self, df):\n",
    "        self.df = df\n",
    "\n",
    "    def adf_test(self, col_list):\n",
    "        for col in col_list:\n",
    "            print(f\"ADF Test for {col}\")\n",
    "            result = adfuller(self.df[col].dropna(), autolag='AIC')\n",
    "            labels = ['ADF test statistic', 'p-value', '# lags used', '# observations']\n",
    "            out = pd.Series(result[0:4], index=labels)\n",
    "\n",
    "            for key, val in result[4].items():\n",
    "                out[f'critical value ({key})'] = val\n",
    "\n",
    "            print(out.to_string())\n",
    "\n",
    "            if result[1] <= 0.05:\n",
    "                print(\"Strong evidence against the null hypothesis\")\n",
    "                print(\"Reject the null hypothesis\")\n",
    "                print(\"Data has no unit root and is stationary\")\n",
    "            else:\n",
    "                print(\"Weak evidence against the null hypothesis\")\n",
    "                print(\"Fail to reject the null hypothesis\")\n",
    "                print(\"Data has a unit root and is non-stationary\")\n",
    "            print()\n",
    "\n",
    "    def fit_model(self, valcol_list):\n",
    "        train = self.df.iloc[:-6, :]\n",
    "        test = self.df.iloc[-6:, :]\n",
    "\n",
    "        print(f\"Training data: {train.shape}, Test data: {test.shape}\")\n",
    "\n",
    "        # Fit a VARMAX model\n",
    "        model = VARMAX(train[valcol_list], order=(0, 1))  # Adjust (p,q) as needed\n",
    "        fitted_model = model.fit(disp=False)\n",
    "\n",
    "        # Forecast\n",
    "        forecast = fitted_model.forecast(steps=len(test))\n",
    "        print(f\"Forecast results:\\n{forecast}\")\n",
    "\n",
    "        # Evaluate model\n",
    "        for col in valcol_list:\n",
    "            mse = mean_squared_error(test[col], forecast[col])\n",
    "            rmse_error = rmse(test[col], forecast[col])\n",
    "            mape = mean_absolute_percentage_error(test[col], forecast[col])\n",
    "            accuracy = (1 - mape) * 100\n",
    "\n",
    "            print(f\"\\nMetrics for {col}:\")\n",
    "            print(f'MSE: {mse}')\n",
    "            print(f'RMSE: {rmse_error}')\n",
    "            print(f'MAPE: {mape}')\n",
    "            print(f'Accuracy: {accuracy}%')\n",
    "\n",
    "        return forecast\n",
    "\n",
    "    def full_data_model(self, valcol_list):\n",
    "        model = VARMAX(self.df[valcol_list], order=(0, 1))  # Adjust (p,q) as needed\n",
    "        fitted_model = model.fit(disp=False)\n",
    "\n",
    "        # Forecast the next 3 periods\n",
    "        forecast = fitted_model.forecast(steps=3)\n",
    "        print(f\"Full Data Forecast results:\\n{forecast}\")\n",
    "\n",
    "        # Create final DataFrame\n",
    "        final_DF = pd.concat([self.df[valcol_list], forecast], axis=0)\n",
    "        final_DF['Type'] = ['Actual'] * len(self.df) + ['Predicted'] * len(forecast)\n",
    "        \n",
    "        print(final_DF.tail(10))\n",
    "        return final_DF\n",
    "\n",
    "# Initialize with the DataFrame containing both CLEAR WATER PUMPING FLOW ML and encoded remarks columns\n",
    "c2 = TSA_Multi(df_monthly)\n",
    "\n",
    "# Perform ADF test for all columns (Clear Water and remarks)\n",
    "columns_to_forecast = ['CLEAR WATER PUMPING FLOW ML'] + list(remarks_encoded_df.columns)\n",
    "c2.adf_test(columns_to_forecast)\n",
    "\n",
    "# Fit and forecast the model\n",
    "forecast_results = c2.fit_model(columns_to_forecast)\n",
    "\n",
    "# Forecast for the full dataset and future periods\n",
    "c2.full_data_model(columns_to_forecast)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_weekly = df.resample('W').sum(numeric_only=True)\n",
    "df_weekly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.statespace.varmax import VARMAX\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "from pmdarima.arima import auto_arima\n",
    "from statsmodels.tsa.arima.model import ARIMA,ARIMAResults\n",
    "from sklearn.metrics import mean_squared_error,mean_absolute_percentage_error\n",
    "from statsmodels.tools.eval_measures import rmse\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "import statsmodels.api as sm\n",
    "\n",
    "class TSA_Multi:\n",
    "    def __init__(self, df):\n",
    "        self.df = df\n",
    "\n",
    "    def adf_test(self, col_list):\n",
    "        for col in col_list:\n",
    "            print(f\"ADF Test for {col}\")\n",
    "            result = adfuller(self.df[col].dropna(), autolag='AIC')\n",
    "            labels = ['ADF test statistic', 'p-value', '# lags used', '# observations']\n",
    "            out = pd.Series(result[0:4], index=labels)\n",
    "\n",
    "            for key, val in result[4].items():\n",
    "                out[f'critical value ({key})'] = val\n",
    "\n",
    "            print(out.to_string())\n",
    "\n",
    "            if result[1] <= 0.05:\n",
    "                print(\"Strong evidence against the null hypothesis\")\n",
    "                print(\"Reject the null hypothesis\")\n",
    "                print(\"Data has no unit root and is stationary\")\n",
    "            else:\n",
    "                print(\"Weak evidence against the null hypothesis\")\n",
    "                print(\"Fail to reject the null hypothesis\")\n",
    "                print(\"Data has a unit root and is non-stationary\")\n",
    "            print()\n",
    "\n",
    "    def fit_model(self, valcol_list):\n",
    "        train = self.df.iloc[:-6, :]\n",
    "        test = self.df.iloc[-6:, :]\n",
    "\n",
    "        print(f\"Training data: {train.shape}, Test data: {test.shape}\")\n",
    "\n",
    "        # Fit a VARMAX model\n",
    "        model = VARMAX(train[valcol_list], order=(0, 1))  # Adjust (p,q) as needed\n",
    "        fitted_model = model.fit(disp=False)\n",
    "\n",
    "        # Forecast\n",
    "        forecast = fitted_model.forecast(steps=len(test))\n",
    "        print(f\"Forecast results:\\n{forecast}\")\n",
    "\n",
    "        # Evaluate model\n",
    "        for col in valcol_list:\n",
    "            mse = mean_squared_error(test[col], forecast[col])\n",
    "            rmse_error = rmse(test[col], forecast[col])\n",
    "            mape = mean_absolute_percentage_error(test[col], forecast[col])\n",
    "            accuracy = (1 - mape) * 100\n",
    "\n",
    "            print(f\"\\nMetrics for {col}:\")\n",
    "            print(f'MSE: {mse}')\n",
    "            print(f'RMSE: {rmse_error}')\n",
    "            print(f'MAPE: {mape}')\n",
    "            print(f'Accuracy: {accuracy}%')\n",
    "\n",
    "        return forecast\n",
    "\n",
    "    def full_data_model(self, valcol_list):\n",
    "        model = VARMAX(self.df[valcol_list], order=(0, 1))  # Adjust (p,q) as needed\n",
    "        fitted_model = model.fit(disp=False)\n",
    "\n",
    "        # Forecast the next 3 periods\n",
    "        forecast = fitted_model.forecast(steps=3)\n",
    "        print(f\"Full Data Forecast results:\\n{forecast}\")\n",
    "\n",
    "        # Create final DataFrame\n",
    "        final_DF = pd.concat([self.df[valcol_list], forecast], axis=0)\n",
    "        final_DF['Type'] = ['Actual'] * len(self.df) + ['Predicted'] * len(forecast)\n",
    "        \n",
    "        print(final_DF.tail(10))\n",
    "        return final_DF\n",
    "\n",
    "# Initialize with the DataFrame containing both CLEAR WATER PUMPING FLOW ML and encoded remarks columns\n",
    "c2 = TSA_Multi(df_weekly)\n",
    "\n",
    "# Perform ADF test for all columns (Clear Water and remarks)\n",
    "columns_to_forecast = ['CLEAR WATER PUMPING FLOW ML'] + list(remarks_encoded_df.columns)\n",
    "c2.adf_test(columns_to_forecast)\n",
    "\n",
    "# Fit and forecast the model\n",
    "forecast_results = c2.fit_model(columns_to_forecast)\n",
    "\n",
    "# Forecast for the full dataset and future periods\n",
    "c2.full_data_model(columns_to_forecast)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
