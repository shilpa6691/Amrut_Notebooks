{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "from pmdarima import auto_arima"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv(\"final_data_in_ML.csv\",parse_dates=['Standardized_Date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Standardized_Date'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['remarks category'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['DATETIME'] = pd.to_datetime(df['Standardized_Date'].astype(str) + ' ' + df['STANDARDIZED_TIME'].astype(str))\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df[['DATETIME','CLEAR WATER PUMPING FLOW ML','REMARKS']]\n",
    "df.set_index('DATETIME', inplace=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "# Time Series Resampling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "Daily Resampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_daily = df.resample('D').sum(numeric_only=True)\n",
    "df_daily"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_daily.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pylab import rcParams\n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.api as sm\n",
    "rcParams['figure.figsize'] = 12, 8\n",
    "\n",
    "decomposition = sm.tsa.seasonal_decompose(df_daily['CLEAR WATER PUMPING FLOW ML'], model='additive')\n",
    "\n",
    "\n",
    "fig = decomposition.plot()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.stattools import adfuller\n",
    "from pmdarima.arima import auto_arima\n",
    "from statsmodels.tsa.arima.model import ARIMA,ARIMAResults\n",
    "from sklearn.metrics import mean_squared_error,mean_absolute_percentage_error\n",
    "from statsmodels.tools.eval_measures import rmse\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "import statsmodels.api as sm\n",
    "class TSA:\n",
    "    def __init__(self,df):\n",
    "        self.df = df\n",
    "        \n",
    "    def adf_test(self,valcol):\n",
    "        \"\"\"\n",
    "        Pass in a time series and an optional title, returns an ADF report\n",
    "        \"\"\"\n",
    "        result = adfuller(self.df[valcol].dropna(),autolag='AIC') # .dropna() handles differenced data\n",
    "    \n",
    "        labels = ['ADF test statistic','p-value','# lags used','# observations']\n",
    "        out = pd.Series(result[0:4],index=labels)\n",
    "\n",
    "        for key,val in result[4].items():\n",
    "            out[f'critical value ({key})']=val\n",
    "        \n",
    "        print(out.to_string())          # .to_string() removes the line \"dtype: float64\"\n",
    "    \n",
    "        if result[1] <= 0.05:\n",
    "            print(\"Strong evidence against the null hypothesis\")\n",
    "            print(\"Reject the null hypothesis\")\n",
    "            print(\"Data has no unit root and is stationary\")\n",
    "            state = \"Stationary\"\n",
    "        else:\n",
    "            print(\"Weak evidence against the null hypothesis\")\n",
    "            print(\"Fail to reject the null hypothesis\")\n",
    "            print(\"Data has a unit root and is non-stationary\")\n",
    "            state = \"Non-stationary\"\n",
    "        if state == \"Stationary\":\n",
    "            return \"Yes\"\n",
    "        else:\n",
    "            return \"No\"\n",
    "    def determine_ARIMA_order(self,valcol):\n",
    "        stepwise_fit = auto_arima(self.df[valcol],\n",
    "#                                   start_p=1, start_q=1,max_p=3,max_q=3,trace=True,\n",
    "                          error_action='ignore',   # we don't want to know if an order does not work\n",
    "                          suppress_warnings=True,  # we don't want convergence warnings\n",
    "                          stepwise=True)           # set to stepwise\n",
    "        \n",
    "        best_order = stepwise_fit.get_params().get('order')\n",
    "        print('The best order is {}'.format(best_order))\n",
    "        return best_order\n",
    "    def fit_model(self,valcol):\n",
    "        train = self.df[valcol][:len(self.df[valcol])-6]\n",
    "        test = self.df[valcol][len(self.df[valcol])-6:len(self.df[valcol])-3]\n",
    "        val = self.df[valcol][len(self.df[valcol])-3:]\n",
    "        # Assuming 'valcol' is the column name containing the data\n",
    "#         train = self.df[self.df.index < '2024-06-30'][valcol]\n",
    "#         val = self.df[(self.df.index >= '2024-06-30') & (self.df.index <= '2024-07-30')][valcol]\n",
    "#         test = self.df[self.df.index > '2024-07-30'][valcol]\n",
    "\n",
    "        start = len(train)\n",
    "        end = len(train)+len(test)-1\n",
    "        print('start : {}'.format(start))\n",
    "        print('end : {}'.format(end))\n",
    "\n",
    "        results = ARIMA(train,order=c1.determine_ARIMA_order(valcol)).fit()\n",
    "#         results = ARIMA(train,order=(2,1,1)).fit()\n",
    "        predictions = results.predict(start=start, end=end)\n",
    "        predictions_test = results.predict(start=start, end=end).round(2)\n",
    "        print(\"predictions_test\",predictions_test)\n",
    "        predictions_val = results.predict(start=end+1, end=len(train)+len(test)+len(val)-1).round(2)\n",
    "\n",
    "        error1 = mean_squared_error(test, predictions)\n",
    "        error2 = rmse(test, predictions)\n",
    "        error3 = mean_absolute_percentage_error(test,predictions)\n",
    "        accuracy = (1-error3)*100\n",
    "        print(f'MSE Error: {error1:11.10}')\n",
    "        print(f'RMSE Error: {error2:11.10}')\n",
    "        print(f'MAPE Error: {error3:11.10}')\n",
    "        print(f'Accuracy: {accuracy:11.10}')\n",
    "        \n",
    "        return predictions_val\n",
    "        \n",
    "            \n",
    "    def full_data_model(self,valcol):\n",
    "        results = ARIMA(self.df[valcol],order=c1.determine_ARIMA_order(valcol)).fit()\n",
    "        fcast = results.predict(len(self.df), len(self.df)+3).round(2)\n",
    "        print(results.summary())\n",
    "        \n",
    "        print(fcast)\n",
    "        DF = pd.DataFrame(self.df[valcol])\n",
    "        DF['Type'] = 'Actual'\n",
    "        DF_fcast = pd.DataFrame(fcast)\n",
    "        DF_fcast['Type'] = 'Predicted'\n",
    "        DF_fcast = DF_fcast.rename(columns={'predicted_mean':valcol})\n",
    "        final_DF = pd.concat([DF,DF_fcast])\n",
    "        final_DF = final_DF.reset_index()\n",
    "\n",
    "        DF_val = pd.DataFrame(c1.fit_model(valcol))\n",
    "        DF_val = DF_val.reset_index()\n",
    "        DF_val = DF_val.rename(columns={'index':'Date','predicted_mean':'Validation'})\n",
    "        final_DF = final_DF.rename(columns={'index':'Date'})\n",
    "        print(DF_val)\n",
    "        print(final_DF)\n",
    "        final_DF =  final_DF.merge(DF_val, on='Date',how='outer')\n",
    "        final_DF['Date'] = final_DF['Date'].astype('str')\n",
    "        print(final_DF)\n",
    "        print(final_DF.tail(60))\n",
    "        \n",
    "c1 = TSA(df_daily)\n",
    "c1.adf_test(\"CLEAR WATER PUMPING FLOW ML\")\n",
    "c1.determine_ARIMA_order(\"CLEAR WATER PUMPING FLOW ML\")\n",
    "c1.fit_model('CLEAR WATER PUMPING FLOW ML')\n",
    "c1.full_data_model('CLEAR WATER PUMPING FLOW ML')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "from prophet import Prophet\n",
    "import pandas as pd\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_percentage_error\n",
    "from statsmodels.tools.eval_measures import rmse\n",
    "\n",
    "class TSA:\n",
    "    def __init__(self, df):\n",
    "        self.df = df\n",
    "\n",
    "    def prepare_data_for_prophet(self, valcol):\n",
    "        # Prepare the dataframe in the format required for Prophet: 'ds' for date and 'y' for values\n",
    "        df_prophet = self.df[[valcol]].reset_index()\n",
    "        df_prophet.columns = ['ds', 'y']\n",
    "        return df_prophet\n",
    "\n",
    "    def fit_prophet(self, valcol):\n",
    "        # Prepare data\n",
    "        df_prophet = self.prepare_data_for_prophet(valcol)\n",
    "\n",
    "        # Split data into train, validation, and test\n",
    "        train = df_prophet.iloc[:-6]\n",
    "        test = df_prophet.iloc[-6:-3]\n",
    "        val = df_prophet.iloc[-3:]\n",
    "\n",
    "        # Fit the model\n",
    "        model = Prophet()\n",
    "        model.fit(train)\n",
    "\n",
    "        # Create future dataframe for predictions\n",
    "        future = model.make_future_dataframe(periods=6)\n",
    "\n",
    "        # Forecast\n",
    "        forecast = model.predict(future)\n",
    "        forecast_test = forecast.iloc[-6:-3]['yhat'].round(2)\n",
    "        forecast_val = forecast.iloc[-3:]['yhat'].round(2)\n",
    "\n",
    "        # Calculate errors\n",
    "        error1 = mean_squared_error(test['y'], forecast_test)\n",
    "        error2 = rmse(test['y'], forecast_test)\n",
    "        error3 = mean_absolute_percentage_error(test['y'], forecast_test)\n",
    "        accuracy = (1 - error3) * 100\n",
    "\n",
    "        print(f'MSE Error: {error1:11.10}')\n",
    "        print(f'RMSE Error: {error2:11.10}')\n",
    "        print(f'MAPE Error: {error3:11.10}')\n",
    "        print(f'Accuracy: {accuracy:11.10}')\n",
    "\n",
    "        return forecast_val\n",
    "\n",
    "    def full_data_prophet(self, valcol):\n",
    "        df_prophet = self.prepare_data_for_prophet(valcol)\n",
    "\n",
    "        # Fit the model to the full dataset\n",
    "        model = Prophet()\n",
    "        model.fit(df_prophet)\n",
    "\n",
    "        # Forecast the next 4 periods\n",
    "        future = model.make_future_dataframe(periods=4)\n",
    "        forecast = model.predict(future)\n",
    "        forecast = forecast[['ds', 'yhat']].tail(4).round(2)\n",
    "\n",
    "        # Print summary and forecast\n",
    "        print(forecast)\n",
    "\n",
    "        # Create a DataFrame for the actual and predicted data\n",
    "        DF = pd.DataFrame(self.df[valcol])\n",
    "        DF['Type'] = 'Actual'\n",
    "        forecast['Type'] = 'Predicted'\n",
    "        forecast = forecast.rename(columns={'yhat': valcol})\n",
    "        final_DF = pd.concat([DF, forecast.set_index('ds')])\n",
    "        final_DF = final_DF.reset_index()\n",
    "\n",
    "        # Add validation data\n",
    "        DF_val = pd.DataFrame(self.fit_prophet(valcol))\n",
    "        DF_val = DF_val.reset_index()\n",
    "        \n",
    "        # Ensure both 'Date' columns are of datetime type before merging\n",
    "        DF_val = DF_val.rename(columns={'index': 'Date', 'yhat': 'Validation'})\n",
    "        DF_val['Date'] = pd.to_datetime(DF_val['Date'])  # Convert to datetime if not already\n",
    "        \n",
    "        final_DF = final_DF.rename(columns={'index': 'Date'})\n",
    "        final_DF['Date'] = pd.to_datetime(final_DF['Date'])  # Convert 'Date' to datetime\n",
    "\n",
    "        # Merge on 'Date' after ensuring both columns are of datetime type\n",
    "        final_DF = final_DF.merge(DF_val, on='Date', how='outer')\n",
    "        final_DF['Date'] = final_DF['Date'].astype('str')  # Convert Date to string if needed for final output\n",
    "\n",
    "        print(final_DF)\n",
    "        print(final_DF.tail(60))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming df_daily is your dataframe and 'CLEAR WATER PUMPING FLOW ML' is the target column\n",
    "c1 = TSA(df_daily)\n",
    "c1.fit_prophet('CLEAR WATER PUMPING FLOW ML')\n",
    "c1.full_data_prophet('CLEAR WATER PUMPING FLOW ML')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18",
   "metadata": {},
   "source": [
    "Weekly Resampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_weekly = df_daily.resample('W')['CLEAR WATER PUMPING FLOW ML'].sum()\n",
    "df_weekly= df_weekly.reset_index()\n",
    "df_weekly= df_weekly.set_index('DATETIME')\n",
    "df_weekly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_weekly.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pylab import rcParams\n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.api as sm\n",
    "rcParams['figure.figsize'] = 12, 8\n",
    "\n",
    "decomposition = sm.tsa.seasonal_decompose(df_weekly['CLEAR WATER PUMPING FLOW ML'], model='additive')\n",
    "\n",
    "\n",
    "fig = decomposition.plot()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.stattools import adfuller\n",
    "from pmdarima.arima import auto_arima\n",
    "from statsmodels.tsa.arima.model import ARIMA,ARIMAResults\n",
    "from sklearn.metrics import mean_squared_error,mean_absolute_percentage_error\n",
    "from statsmodels.tools.eval_measures import rmse\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "import statsmodels.api as sm\n",
    "class TSA:\n",
    "    def __init__(self,df):\n",
    "        self.df = df\n",
    "        \n",
    "    def adf_test(self,valcol):\n",
    "        \"\"\"\n",
    "        Pass in a time series and an optional title, returns an ADF report\n",
    "        \"\"\"\n",
    "        result = adfuller(self.df[valcol].dropna(),autolag='AIC') # .dropna() handles differenced data\n",
    "    \n",
    "        labels = ['ADF test statistic','p-value','# lags used','# observations']\n",
    "        out = pd.Series(result[0:4],index=labels)\n",
    "\n",
    "        for key,val in result[4].items():\n",
    "            out[f'critical value ({key})']=val\n",
    "        \n",
    "        print(out.to_string())          # .to_string() removes the line \"dtype: float64\"\n",
    "    \n",
    "        if result[1] <= 0.05:\n",
    "            print(\"Strong evidence against the null hypothesis\")\n",
    "            print(\"Reject the null hypothesis\")\n",
    "            print(\"Data has no unit root and is stationary\")\n",
    "            state = \"Stationary\"\n",
    "        else:\n",
    "            print(\"Weak evidence against the null hypothesis\")\n",
    "            print(\"Fail to reject the null hypothesis\")\n",
    "            print(\"Data has a unit root and is non-stationary\")\n",
    "            state = \"Non-stationary\"\n",
    "        if state == \"Stationary\":\n",
    "            return \"Yes\"\n",
    "        else:\n",
    "            return \"No\"\n",
    "    def determine_ARIMA_order(self,valcol):\n",
    "        stepwise_fit = auto_arima(self.df[valcol],\n",
    "#                                   start_p=1, start_q=1,max_p=3,max_q=3,trace=True,\n",
    "                          error_action='ignore',   # we don't want to know if an order does not work\n",
    "                          suppress_warnings=True,  # we don't want convergence warnings\n",
    "                          stepwise=True)           # set to stepwise      \n",
    "        best_order = stepwise_fit.get_params().get('order')\n",
    "        print('The best order is {}'.format(best_order))\n",
    "        return best_order\n",
    "    def fit_model(self,valcol):\n",
    "        train = self.df[valcol][:len(self.df[valcol])-8]\n",
    "        test = self.df[valcol][len(self.df[valcol])-8:len(self.df[valcol])-4]\n",
    "        val = self.df[valcol][len(self.df[valcol])-4:]\n",
    "        # Assuming 'valcol' is the column name containing the data\n",
    "#         train = self.df[self.df.index < '2024-06-30'][valcol]\n",
    "#         val = self.df[(self.df.index >= '2024-06-30') & (self.df.index <= '2024-07-30')][valcol]\n",
    "#         test = self.df[self.df.index > '2024-07-30'][valcol]\n",
    "        start = len(train)\n",
    "        end = len(train)+len(test)-1\n",
    "        print('start : {}'.format(start))\n",
    "        print('end : {}'.format(end))\n",
    "        results = ARIMA(train,order=c1.determine_ARIMA_order(valcol),trend='t').fit()\n",
    "#         results = ARIMA(train,order=c1.determine_ARIMA_order(valcol)).fit()\n",
    "        predictions = results.predict(start=start, end=end)\n",
    "        predictions_test = results.predict(start=start, end=end).round(2)\n",
    "        predictions_val = results.predict(start=end+1, end=len(train)+len(test)+len(val)-1).round(2)\n",
    "        \n",
    "        error1 = mean_squared_error(test, predictions)\n",
    "        error2 = rmse(test, predictions)\n",
    "        error3 = mean_absolute_percentage_error(test,predictions)\n",
    "        accuracy = (1-error3)*100\n",
    "        print(f'MSE Error: {error1:11.10}')\n",
    "        print(f'RMSE Error: {error2:11.10}')\n",
    "        print(f'MAPE Error: {error3:11.10}')\n",
    "        print(f'Accuracy: {accuracy:11.10}')\n",
    "\n",
    "        return predictions_val\n",
    "        \n",
    "            \n",
    "    def full_data_model(self,valcol):\n",
    "        results = ARIMA(self.df[valcol],order=c1.determine_ARIMA_order(valcol),trend='t').fit()\n",
    "        fcast = results.predict(len(self.df), len(self.df)+3).round(2)\n",
    "        print(fcast)\n",
    "        DF = pd.DataFrame(self.df[valcol])\n",
    "        DF['Type'] = 'Actual'\n",
    "        DF_fcast = pd.DataFrame(fcast)\n",
    "        DF_fcast['Type'] = 'Predicted'\n",
    "        DF_fcast = DF_fcast.rename(columns={'predicted_mean':valcol})\n",
    "        final_DF = pd.concat([DF,DF_fcast])\n",
    "        final_DF = final_DF.reset_index()\n",
    "\n",
    "        DF_val = pd.DataFrame(c1.fit_model(valcol))\n",
    "        DF_val = DF_val.reset_index()\n",
    "        DF_val = DF_val.rename(columns={'index':'Date','predicted_mean':'Validation'})\n",
    "        final_DF = final_DF.rename(columns={'index':'Date'})\n",
    "        print(DF_val)\n",
    "        print(final_DF)\n",
    "        final_DF =  final_DF.merge(DF_val, on='Date',how='outer')\n",
    "        final_DF['Date'] = final_DF['Date'].astype('str')\n",
    "        print(final_DF)\n",
    "        print(final_DF.tail(50))\n",
    "        \n",
    "c1 = TSA(df_weekly)\n",
    "c1.adf_test(\"CLEAR WATER PUMPING FLOW ML\")\n",
    "c1.determine_ARIMA_order(\"CLEAR WATER PUMPING FLOW ML\")\n",
    "c1.fit_model('CLEAR WATER PUMPING FLOW ML')\n",
    "c1.full_data_model('CLEAR WATER PUMPING FLOW ML')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "25",
   "metadata": {},
   "source": [
    "Monthly Resampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_monthly = df.resample('M')['CLEAR WATER PUMPING FLOW ML'].sum()\n",
    "df_monthly= df_monthly.reset_index()\n",
    "df_monthly= df_monthly.set_index('DATETIME')\n",
    "df_monthly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_monthly.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Calculate and plot the trend using a rolling average (e.g., 6 months)\n",
    "df_monthly['Trend'] = df_monthly['CLEAR WATER PUMPING FLOW ML'].rolling(window=6).mean()\n",
    "\n",
    "# Create the figure and axis objects\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "# Plot the original data with a specified color (e.g., blue)\n",
    "df_monthly['CLEAR WATER PUMPING FLOW ML'].plot(ax=ax, label='Monthly Flow', color='blue')\n",
    "\n",
    "# Plot the trend line with a different color (e.g., orange)\n",
    "df_monthly['Trend'].plot(ax=ax, label='Trend (6-month Rolling Avg)', color='orange')\n",
    "\n",
    "# Add labels, title, and legend\n",
    "plt.title('Monthly CLEAR WATER PUMPING FLOW ML with Trend')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('CLEAR WATER PUMPING FLOW ML')\n",
    "plt.legend()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pylab import rcParams\n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.api as sm\n",
    "rcParams['figure.figsize'] = 12, 8\n",
    "\n",
    "decomposition = sm.tsa.seasonal_decompose(df_monthly['CLEAR WATER PUMPING FLOW ML'], model='additive')\n",
    "\n",
    "\n",
    "fig = decomposition.plot()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.stattools import adfuller\n",
    "from pmdarima.arima import auto_arima\n",
    "from statsmodels.tsa.arima.model import ARIMA,ARIMAResults\n",
    "from sklearn.metrics import mean_squared_error,mean_absolute_percentage_error\n",
    "from statsmodels.tools.eval_measures import rmse\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "import statsmodels.api as sm\n",
    "class TSA:\n",
    "    def __init__(self,df):\n",
    "        self.df = df\n",
    "        \n",
    "    def adf_test(self,valcol):\n",
    "        \"\"\"\n",
    "        Pass in a time series and an optional title, returns an ADF report\n",
    "        \"\"\"\n",
    "        result = adfuller(self.df[valcol].dropna(),autolag='AIC') # .dropna() handles differenced data\n",
    "    \n",
    "        labels = ['ADF test statistic','p-value','# lags used','# observations']\n",
    "        out = pd.Series(result[0:4],index=labels)\n",
    "\n",
    "        for key,val in result[4].items():\n",
    "            out[f'critical value ({key})']=val\n",
    "        \n",
    "        print(out.to_string())          # .to_string() removes the line \"dtype: float64\"\n",
    "    \n",
    "        if result[1] <= 0.05:\n",
    "            print(\"Strong evidence against the null hypothesis\")\n",
    "            print(\"Reject the null hypothesis\")\n",
    "            print(\"Data has no unit root and is stationary\")\n",
    "            state = \"Stationary\"\n",
    "        else:\n",
    "            print(\"Weak evidence against the null hypothesis\")\n",
    "            print(\"Fail to reject the null hypothesis\")\n",
    "            print(\"Data has a unit root and is non-stationary\")\n",
    "            state = \"Non-stationary\"\n",
    "        if state == \"Stationary\":\n",
    "            return \"Yes\"\n",
    "        else:\n",
    "            return \"No\"\n",
    "    def determine_ARIMA_order(self,valcol):\n",
    "        stepwise_fit = auto_arima(self.df[valcol],\n",
    "#                                   start_p=1, start_q=1,max_p=3,max_q=3,trace=True,\n",
    "                          error_action='ignore',   # we don't want to know if an order does not work\n",
    "                          suppress_warnings=True,  # we don't want convergence warnings\n",
    "                          stepwise=True)           # set to stepwise\n",
    "        best_order = stepwise_fit.get_params().get('order')\n",
    "        print('The best order is {}'.format(best_order))\n",
    "        return best_order\n",
    "    def fit_model(self,valcol):\n",
    "        train = self.df[valcol][:len(self.df[valcol])-8]\n",
    "        test = self.df[valcol][len(self.df[valcol])-8:len(self.df[valcol])-4]\n",
    "        val = self.df[valcol][len(self.df[valcol])-4:]\n",
    "        # Assuming 'valcol' is the column name containing the data\n",
    "#         train = self.df[self.df.index < '2024-06-30'][valcol]\n",
    "#         val = self.df[(self.df.index >= '2024-06-30') & (self.df.index <= '2024-07-30')][valcol]\n",
    "#         test = self.df[self.df.index > '2024-07-30'][valcol]\n",
    "        start = len(train)\n",
    "        end = len(train)+len(test)-1\n",
    "        print('start : {}'.format(start))\n",
    "        print('end : {}'.format(end))\n",
    "#         results = ARIMA(train,order=c1.determine_ARIMA_order(valcol)).fit()\n",
    "        results = ARIMA(train,order=c1.determine_ARIMA_order(valcol),trend='t').fit()\n",
    "        predictions = results.predict(start=start, end=end)\n",
    "        predictions_test = results.predict(start=start, end=end).round(2)\n",
    "        predictions_val = results.predict(start=end+1, end=len(train)+len(test)+len(val)-1).round(2)\n",
    "\n",
    "        error1 = mean_squared_error(test, predictions)\n",
    "        error2 = rmse(test, predictions)\n",
    "        error3 = mean_absolute_percentage_error(test,predictions)\n",
    "        accuracy = (1-error3)*100\n",
    "        print(f'MSE Error: {error1:11.10}')\n",
    "        print(f'RMSE Error: {error2:11.10}')\n",
    "        print(f'MAPE Error: {error3:11.10}')\n",
    "        print(f'Accuracy: {accuracy:11.10}')\n",
    "        \n",
    "        return predictions_val\n",
    "        \n",
    "            \n",
    "    def full_data_model(self,valcol):\n",
    "        results = ARIMA(self.df[valcol],order=c1.determine_ARIMA_order(valcol),trend='t').fit()\n",
    "#         if len(self.df[valcol]) > 70:\n",
    "        fcast = results.predict(len(self.df), len(self.df)+3).round(2)\n",
    "        \n",
    "#         else:\n",
    "#             fcast = results.predict(len(self.df), len(self.df)+3,typ='levels').round(2)\n",
    "#         ax = self.df[valcol].plot(legend=True,figsize=(12,6))\n",
    "#         fcast.plot(legend=True)\n",
    "        print(fcast)\n",
    "        DF = pd.DataFrame(self.df[valcol])\n",
    "        DF['Type'] = 'Actual'\n",
    "       \n",
    "        \n",
    "#         DF = DF.reset_index()\n",
    "        DF_fcast = pd.DataFrame(fcast)\n",
    "        DF_fcast['Type'] = 'Predicted'\n",
    "        DF_fcast = DF_fcast.rename(columns={'predicted_mean':valcol})\n",
    "        final_DF = pd.concat([DF,DF_fcast])\n",
    "        final_DF = final_DF.reset_index()\n",
    "#         DF_fcast = DF_fcast.rename(columns={'predicted_mean':'Predicted', 'index':'DateTime'})\n",
    "        DF_val = pd.DataFrame(c1.fit_model(valcol))\n",
    "        DF_val = DF_val.reset_index()\n",
    "        DF_val = DF_val.rename(columns={'index':'Date','predicted_mean':'Validation'})\n",
    "        final_DF = final_DF.rename(columns={'index':'Date'})\n",
    "        print(DF_val)\n",
    "        print(final_DF)\n",
    "#         final_DF =  final_DF.merge(DF_val, on='DateTime',how='outer')\n",
    "        final_DF =  final_DF.merge(DF_val, on='Date',how='outer')\n",
    "        final_DF['Date'] = final_DF['Date'].astype('str')\n",
    "        print(final_DF)\n",
    "        print(final_DF.tail(50))\n",
    "        \n",
    "c1 = TSA(df_monthly)\n",
    "c1.adf_test(\"CLEAR WATER PUMPING FLOW ML\")\n",
    "c1.determine_ARIMA_order(\"CLEAR WATER PUMPING FLOW ML\")\n",
    "c1.fit_model('CLEAR WATER PUMPING FLOW ML')\n",
    "c1.full_data_model('CLEAR WATER PUMPING FLOW ML')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
