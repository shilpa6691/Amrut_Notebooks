{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "from pmdarima.arima import auto_arima\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "from statsmodels.tsa.arima.model import ARIMA,ARIMAResults\n",
    "from sklearn.metrics import mean_squared_error,mean_absolute_percentage_error\n",
    "from statsmodels.tools.eval_measures import rmse\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import pickle\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "from pmdarima.arima.utils import nsdiffs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install --upgrade statsmodels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_weekly= pd.read_csv('weekly_AQI (1).csv',index_col='DateTime',parse_dates=True)\n",
    "df_weekly.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_weekly.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = df_weekly['AQI'].mean()\n",
    "std_dev = df_weekly['AQI'].std()\n",
    "\n",
    "# Calculate lower and upper bounds using Z-score method\n",
    "lower_limit = mean - 3 * std_dev  \n",
    "upper_limit = mean + 3 * std_dev \n",
    "\n",
    "# Identify outliers\n",
    "outliers_before = df_weekly[(df_weekly['AQI'] < lower_limit) | (df_weekly['AQI'] > upper_limit)]\n",
    "\n",
    "# Print outliers before ffill\n",
    "print(f\"Lower Limit: {lower_limit}\")\n",
    "print(f\"Upper Limit: {upper_limit}\")\n",
    "print('No. of outliers before ffill:', len(outliers_before))\n",
    "print(outliers_before)\n",
    "print(\"\\n\")\n",
    "\n",
    "# Forward fill outliers\n",
    "df_weekly.loc[outliers_before.index, 'AQI'] = 0\n",
    "df_weekly['AQI'] = df_weekly['AQI'].ffill()\n",
    "\n",
    "# Identify outliers after ffill\n",
    "outliers_after = df_weekly[(df_weekly['AQI'] < lower_limit) | (df_weekly['AQI'] > upper_limit)]\n",
    "\n",
    "# Print outliers after ffill\n",
    "print('No. of outliers after ffill:', len(outliers_after))\n",
    "print(outliers_after)\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_weekly.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_weekly.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_weekly.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filtered = df_weekly[df_weekly['id'] != 'ID023']\n",
    "df_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filtered.id.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TSA:\n",
    "    def __init__(self,df,idcol,loc):\n",
    "        self.df = df\n",
    "        self.idcol = idcol\n",
    "        self.loc = loc\n",
    "        \n",
    "        \n",
    "    def zone_df(self):\n",
    "        self.df = self.df[self.df[self.idcol]== self.loc]\n",
    "        print(self.loc)\n",
    "    \n",
    "    def adf_test(self,valcol):\n",
    "        \"\"\"\n",
    "        Pass in a time series and an optional title, returns an ADF report\n",
    "        \"\"\"\n",
    "        print('Testing')\n",
    "        result = adfuller(self.df[valcol].dropna(),autolag='AIC') # .dropna() handles differenced data\n",
    "        print('result')\n",
    "    \n",
    "        labels = ['ADF test statistic','p-value','# lags used','# observations']\n",
    "        out = pd.Series(result[0:4],index=labels)\n",
    "\n",
    "        for key,val in result[4].items():\n",
    "            out[f'critical value ({key})']=val\n",
    "        \n",
    "        print(out.to_string())          # .to_string() removes the line \"dtype: float64\"\n",
    "    \n",
    "        if result[1] <= 0.05:\n",
    "            print(\"Strong evidence against the null hypothesis\")\n",
    "            print(\"Reject the null hypothesis\")\n",
    "            print(\"Data has no unit root and is stationary\")\n",
    "        else:\n",
    "            print(\"Weak evidence against the null hypothesis\")\n",
    "            print(\"Fail to reject the null hypothesis\")\n",
    "            print(\"Data has a unit root and is non-stationary\")\n",
    "    def determine_ARIMA_order(self,valcol):\n",
    "        stepwise_fit = auto_arima(self.df[valcol], start_p=0, start_q=0,\n",
    "                          error_action='ignore',   # we don't want to know if an order does not work\n",
    "                          suppress_warnings=True,  # we don't want convergence warnings\n",
    "                          stepwise=True)           # set to stepwise\n",
    "        best_order = stepwise_fit.get_params().get('order')\n",
    "        print('The best order is '.format(best_order))\n",
    "        return best_order\n",
    "    def fit_model(self,valcol):\n",
    "#         if len(self.df[valcol]) > 70:\n",
    "        train = self.df[valcol][:len(self.df[valcol])-8]\n",
    "        test = self.df[valcol][len(self.df[valcol])-8:len(self.df[valcol])-4]\n",
    "        val = self.df[valcol][len(self.df[valcol])-4:]\n",
    "#         else:\n",
    "#             train = self.df[valcol][:len(self.df[valcol])-4]\n",
    "#             test = self.df[valcol][len(self.df[valcol])-4:]\n",
    "        start = len(train)\n",
    "        end = len(train)+len(test)-1\n",
    "#             print('train : {}'.format(train))\n",
    "#             print('test : {}'.format(test))\n",
    "        print('start : {}'.format(start))\n",
    "        print('end : {}'.format(end))\n",
    "        results = ARIMA(train,order=c1.determine_ARIMA_order(valcol)).fit()\n",
    "        predictions = results.predict(start=start, end=end)\n",
    "        error1 = mean_squared_error(test, predictions)\n",
    "        error2 = rmse(test, predictions)\n",
    "        print(f'MSE Error: {error1:11.10}')\n",
    "        print(f'RMSE Error: {error2:11.10}')\n",
    "            \n",
    "    def full_data_model(self,valcol):\n",
    "        results = ARIMA(self.df[valcol],order=c1.determine_ARIMA_order(valcol)).fit()\n",
    "        \n",
    "#         with open('TSA_AQI_{}.pkl'.format(self.loc),'wb')as f:\n",
    "#             pickle.dump(results,f)\n",
    "            \n",
    "        order=c1.determine_ARIMA_order(valcol)\n",
    "#         with open('Order_TSA_AQI_{}.pkl'.format(self.loc),'wb')as f:\n",
    "#             pickle.dump(order,f)\n",
    "            \n",
    "#         if len(self.df[valcol]) > 70:\n",
    "        fcast = results.predict(len(self.df), len(self.df)+3,typ='levels').round(2)\n",
    "#         else:\n",
    "#             fcast = results.predict(len(self.df), len(self.df)+3,typ='levels').round(2)\n",
    "#         ax = self.df[valcol].plot(legend=True,figsize=(12,6))\n",
    "#         fcast.plot(legend=True)\n",
    "        print(order)\n",
    "        print(fcast)\n",
    "        DF = pd.DataFrame(self.df[valcol])\n",
    "        DF['Type'] = 'Actual'\n",
    "        DF_fcast = pd.DataFrame(fcast)\n",
    "        DF_fcast = DF_fcast.rename(columns={'predicted_mean':'AQI'})\n",
    "        DF_fcast['Type'] = 'Predicted'\n",
    "        final_DF =  pd.concat([DF,DF_fcast], ignore_index=True)\n",
    "        final_DF = final_DF.reset_index()\n",
    "        final_DF = final_DF.rename(columns={'index':'Date'})\n",
    "        print(final_DF)\n",
    "        #final_DF.to_json('/Users/nithingopinath/Desktop/Bayesian Ways/AQI Deployment\\{}.json'.format(item),orient='records')\n",
    "\n",
    "id_list = list(df_filtered.id.unique())\n",
    "for item in id_list:\n",
    "    c1 = TSA(df_weekly,'id',item)\n",
    "    c1.zone_df()\n",
    "    c1.adf_test('AQI')\n",
    "    c1.determine_ARIMA_order('AQI')\n",
    "    c1.fit_model('AQI')\n",
    "    c1.full_data_model('AQI')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#pickle_out = open(\"model.pkl\",\"wb\")\n",
    "#pickle.dump(results, pickle_out)\n",
    "#pickle_out.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Arima_model = ARIMA(random_state=0).fit()\n",
    "#Arima_model = ARIMA(train,order=c1.determine_ARIMA_order(valcol)).fit()\n",
    "#pickle.dump(Arima_model, open(\"/Users/nithingopinath/Desktop/Bayesian Ways/AQI Deployment using fastapi/ArimaModel.pkl\",\"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "# SARIMA Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TSA_seasonal:\n",
    "    def __init__(self, df,idcol,loc):\n",
    "        self.df = df\n",
    "        self.idcol = idcol\n",
    "        self.loc = loc\n",
    "    \n",
    "    def zone_df(self):\n",
    "        self.df = self.df[self.df[self.idcol]== self.loc]\n",
    "        print(self.loc)\n",
    "        \n",
    "#     def find_D(self,valcol):\n",
    "#          # estimate number of seasonal differences using a Canova-Hansen test\n",
    "#         D = nsdiffs(self.df[valcol],m=12,\n",
    "#             test='ch')  # -> 0\n",
    "#         return D\n",
    "\n",
    "    def adf_test(self, valcol):\n",
    "        \"\"\"\n",
    "        Pass in a time series and an optional title, returns an ADF report\n",
    "        \"\"\"\n",
    "        result = adfuller(self.df[valcol].dropna(), autolag='AIC')  # .dropna() handles differenced data\n",
    "\n",
    "        labels = ['ADF test statistic', 'p-value', '# lags used', '# observations']\n",
    "        out = pd.Series(result[0:4], index=labels)\n",
    "\n",
    "        for key, val in result[4].items():\n",
    "            out[f'critical value ({key})'] = val\n",
    "\n",
    "        if result[1] <= 0.05:\n",
    "            state = \"Stationary\"\n",
    "        else:\n",
    "            state = \"Non-stationary\"\n",
    "        return state\n",
    "\n",
    "    def determine_SARIMA_order(self, valcol):\n",
    "#         D = self.find_D(valcol)\n",
    "        stepwise_fit = auto_arima(self.df[valcol], seasonal=True, m=12,\n",
    "                               start_p=0, start_q=0,\n",
    "#                               start_P=0, start_Q=0,\n",
    "#                                   D=D,\n",
    "                                  error_action='ignore',  # we don't want to know if an order does not work\n",
    "                                  suppress_warnings=True,  # we don't want convergence warnings\n",
    "                                  stepwise=True)  # set to stepwise\n",
    "        best_order = stepwise_fit.get_params().get('order')\n",
    "        best_seasonal_order = stepwise_fit.get_params().get('seasonal_order')\n",
    "        print('The best seasonal order is {}'.format(best_seasonal_order))\n",
    "        print('The best order is {}'.format(best_order))\n",
    "        return best_order, best_seasonal_order\n",
    "\n",
    "    def fit_model(self, valcol):\n",
    "    # Split the data into train, test, and validation sets\n",
    "        train = self.df[valcol][:len(self.df[valcol]) - 8]\n",
    "        test = self.df[valcol][len(self.df[valcol]) - 8:len(self.df[valcol]) - 4]\n",
    "        val = self.df[valcol][len(self.df[valcol]) - 4:]\n",
    "\n",
    "        # Determine the best SARIMA order\n",
    "        best_order, best_seasonal_order = self.determine_SARIMA_order(valcol)\n",
    "\n",
    "        # Fit the SARIMA model on the training data\n",
    "        model = SARIMAX(train, order=best_order, seasonal_order=best_seasonal_order)\n",
    "        results = model.fit()\n",
    "\n",
    "        # Generate predictions for the test set\n",
    "        predictions = results.predict(start=len(train), end=len(train) + len(test) - 1)\n",
    "\n",
    "        # Generate predictions for the validation set\n",
    "        predictions_val = results.predict(start=len(train) + len(test), end=len(train) + len(test) + len(val) - 1)\n",
    "\n",
    "        # Calculate error metrics for the test set\n",
    "        error1 = mean_squared_error(test, predictions)\n",
    "        error2 = mean_squared_error(test, predictions, squared=False)  # RMSE\n",
    "        error3 = mean_absolute_percentage_error(test, predictions)\n",
    "        accuracy = (1 - error3) * 100\n",
    "        print(f'MSE Error: {error1:11.10}')\n",
    "        print(f'RMSE Error: {error2:11.10}')\n",
    "        print(f'MAPE Error: {error3:11.10}')\n",
    "        print(f'Accuracy: {accuracy:11.10}')\n",
    "        return predictions_val\n",
    "\n",
    "    def full_data_model(self, valcol):\n",
    "        best_order, best_seasonal_order = self.determine_SARIMA_order(valcol)\n",
    "        model = SARIMAX(self.df[valcol], order=best_order, seasonal_order=best_seasonal_order)\n",
    "        results = model.fit()\n",
    "        fcast = results.forecast(steps=4).round(2)  # Forecast 4 steps ahead\n",
    "#         print(fcast)\n",
    "#         fcast_index = pd.date_range(start=self.df.index[-1], periods=4 + 1, freq='M')[1:]  # Assuming monthly data\n",
    "        DF_fcast = pd.DataFrame({valcol: fcast})\n",
    "        DF_fcast['Type'] = 'Predicted'\n",
    "        print(DF_fcast)\n",
    "        DF = pd.DataFrame(self.df[valcol])\n",
    "        DF['Type'] = 'Actual'\n",
    "        # Concatenate original data and forecast data\n",
    "        combined_DF = pd.concat([DF, DF_fcast])\n",
    "        combined_DF = combined_DF.reset_index().rename(columns={'index':'Date'})\n",
    "        print(combined_DF)\n",
    "        DF_val = pd.DataFrame(c2.fit_model(valcol))\n",
    "        DF_val = DF_val.reset_index()\n",
    "        DF_val = DF_val.rename(columns={'index':'Date','predicted_mean':'Validation'})\n",
    "#         combined_DF = combined_DF.rename(columns={'index':'Date'})\n",
    "        print(DF_val)\n",
    "        print(combined_DF)\n",
    "#         final_DF =  final_DF.merge(DF_val, on='DateTime',how='outer')\n",
    "        final_DF =  combined_DF.merge(DF_val, on='Date',how='outer')\n",
    "        final_DF['Date'] = final_DF['Date'].astype('str')\n",
    "#         print(combined_df)\n",
    "        return final_DF\n",
    "\n",
    "id_list = list(df_filtered.id.unique())\n",
    "for item in id_list:\n",
    "    c2 = TSA_seasonal(df_weekly,'id',item)\n",
    "    c2.zone_df()\n",
    "#     c2.find_D('AQI')\n",
    "    c2.adf_test('AQI')\n",
    "    c2.determine_SARIMA_order('AQI')\n",
    "    c2.fit_model('AQI')\n",
    "    c2.full_data_model('AQI')\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
