{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv('Cleaned_Pattuvam_historic_data_till_July.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df[['DATETIME','TREATED WATER PRODUCTION IN ML']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['DATETIME'] = pd.to_datetime(df['DATETIME'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.set_index('DATETIME',inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.resample('W')['TREATED WATER PRODUCTION IN ML'].mean()\n",
    "df = df.reset_index()\n",
    "df = df.set_index('DATETIME')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decompose the time series\n",
    "decomposition = seasonal_decompose(df['TREATED WATER PRODUCTION IN ML'], model='additive')\n",
    "\n",
    "# Extracting the components\n",
    "trend = decomposition.trend\n",
    "seasonal = decomposition.seasonal\n",
    "residual = decomposition.resid\n",
    "\n",
    "trend_variance = trend.var()\n",
    "seasonal_variance = decomposition.seasonal.var()\n",
    "residual_variance = residual.var()\n",
    "\n",
    "total_variance = trend_variance + seasonal_variance + residual_variance\n",
    "\n",
    "print(f\"Trend Variance: {trend_variance / total_variance:.2%}\")\n",
    "print(f\"Seasonal Variance: {seasonal_variance / total_variance:.2%}\")\n",
    "print(f\"Residual Variance: {residual_variance / total_variance:.2%}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pylab import rcParams\n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.api as sm\n",
    "rcParams['figure.figsize'] = 12, 8\n",
    "\n",
    "decomposition = sm.tsa.seasonal_decompose(df['TREATED WATER PRODUCTION IN ML'], model='additive')\n",
    "\n",
    "\n",
    "fig = decomposition.plot()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.stattools import adfuller\n",
    "from pmdarima.arima import auto_arima\n",
    "from statsmodels.tsa.arima.model import ARIMA,ARIMAResults\n",
    "from sklearn.metrics import mean_squared_error,mean_absolute_percentage_error\n",
    "from statsmodels.tools.eval_measures import rmse\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "import statsmodels.api as sm\n",
    "class TSA:\n",
    "    def __init__(self,df):\n",
    "        self.df = df\n",
    "        \n",
    "    def adf_test(self,valcol):\n",
    "        \"\"\"\n",
    "        Pass in a time series and an optional title, returns an ADF report\n",
    "        \"\"\"\n",
    "        result = adfuller(self.df[valcol].dropna(),autolag='AIC') # .dropna() handles differenced data\n",
    "    \n",
    "        labels = ['ADF test statistic','p-value','# lags used','# observations']\n",
    "        out = pd.Series(result[0:4],index=labels)\n",
    "\n",
    "        for key,val in result[4].items():\n",
    "            out[f'critical value ({key})']=val\n",
    "        \n",
    "        print(out.to_string())          # .to_string() removes the line \"dtype: float64\"\n",
    "    \n",
    "        if result[1] <= 0.05:\n",
    "            print(\"Strong evidence against the null hypothesis\")\n",
    "            print(\"Reject the null hypothesis\")\n",
    "            print(\"Data has no unit root and is stationary\")\n",
    "            state = \"Stationary\"\n",
    "        else:\n",
    "            print(\"Weak evidence against the null hypothesis\")\n",
    "            print(\"Fail to reject the null hypothesis\")\n",
    "            print(\"Data has a unit root and is non-stationary\")\n",
    "            state = \"Non-stationary\"\n",
    "        if state == \"Stationary\":\n",
    "            return \"Yes\"\n",
    "        else:\n",
    "            return \"No\"\n",
    "    def determine_ARIMA_order(self,valcol):\n",
    "        stepwise_fit = auto_arima(self.df[valcol],start_p=1, start_q=1,max_p=3,max_q=3,trace=True,\n",
    "                          error_action='ignore',   # we don't want to know if an order does not work\n",
    "                          suppress_warnings=True,  # we don't want convergence warnings\n",
    "                          stepwise=True)           # set to stepwise\n",
    "        \n",
    "#         print(stepwise_fit.summary())\n",
    "# #         \n",
    "        best_order = stepwise_fit.get_params().get('order')\n",
    "        best_seasonal_order = stepwise_fit.get_params().get('seasonal_order')\n",
    "\n",
    "#         best_order= (1,1,1)\n",
    "        print('The best order is {}'.format(best_order))\n",
    "        print('The best seasonal order is {}'.format(best_seasonal_order))\n",
    "\n",
    "        return best_order\n",
    "    def fit_model(self,valcol):\n",
    "#         if len(self.df[valcol]) > 70:\n",
    "#         train = self.df[valcol][:len(self.df[valcol])-8]\n",
    "#         test = self.df[valcol][len(self.df[valcol])-8:len(self.df[valcol])-4]\n",
    "#         val = self.df[valcol][len(self.df[valcol])-4:]\n",
    "        # Assuming 'valcol' is the column name containing the data\n",
    "        train = self.df[self.df.index < '2024-01-07'][valcol]\n",
    "        val = self.df[(self.df.index >= '2024-01-07') & (self.df.index <= '2024-05-05')][valcol]\n",
    "        test = self.df[self.df.index > '2024-05-05'][valcol]\n",
    "\n",
    "\n",
    "#             train = self.df[valcol][:len(self.df[valcol])-8]\n",
    "#             test = self.df[valcol][len(self.df[valcol])-8:len(self.df[valcol])-4]\n",
    "#             val = self.df[valcol][len(self.df[valcol])-4:]\n",
    "        start = len(train)\n",
    "        end = len(train)+len(test)-1\n",
    "#             print('train : {}'.format(train))\n",
    "#             print('test : {}'.format(test))\n",
    "        print('start : {}'.format(start))\n",
    "        print('end : {}'.format(end))\n",
    "#         results = ARIMA(train,order=c1.determine_ARIMA_order(valcol)).fit()\n",
    "        results = ARIMA(train,order=c1.determine_ARIMA_order(valcol),trend='t').fit()\n",
    "        predictions = results.predict(start=start, end=end)\n",
    "        predictions_val = results.predict(start=end+1, end=len(train)+len(test)+len(val)-1)\n",
    "        error1 = mean_squared_error(test, predictions)\n",
    "        error2 = rmse(test, predictions)\n",
    "        error3 = mean_absolute_percentage_error(test,predictions)\n",
    "        accuracy = (1-error3)*100\n",
    "        print(f'MSE Error: {error1:11.10}')\n",
    "        print(f'RMSE Error: {error2:11.10}')\n",
    "        print(f'MAPE Error: {error3:11.10}')\n",
    "        print(f'Accuracy: {accuracy:11.10}')\n",
    "#         return start, len(test)\n",
    "        model_results = {\"Model\":\"ARIMA\",\"Stationary\": c1.adf_test(valcol),\"X_train\": str(len(train))+\" Weeks\", \"X_test\": str(len(test))+\" Weeks\", \"X_validation\": str(len(val))+\" Weeks\", \"ARIMA_order\": c1.determine_ARIMA_order(valcol), \"MSE\": error1,\"RMSE\": error2,\"MAPE\":error3, \"Accuracy\":((1-error3)*100).round(0)}\n",
    "#         with open('TSA_AQI_model_result_new.json','a') as f:\n",
    "#             f.write(str(model_results)+',')\n",
    "#             f.close()\n",
    "        return predictions_val\n",
    "        \n",
    "            \n",
    "    def full_data_model(self,valcol):\n",
    "        results = ARIMA(self.df[valcol],order=c1.determine_ARIMA_order(valcol)).fit()\n",
    "#         if len(self.df[valcol]) > 70:\n",
    "        fcast = results.predict(len(self.df), len(self.df)+3,typ='levels').round(2)\n",
    "#         else:\n",
    "#             fcast = results.predict(len(self.df), len(self.df)+3,typ='levels').round(2)\n",
    "#         ax = self.df[valcol].plot(legend=True,figsize=(12,6))\n",
    "#         fcast.plot(legend=True)\n",
    "        print(fcast)\n",
    "#         error_rmse = rmse(self.df[valcol],fcast[0:len(self.df)])\n",
    "#         error_mse = mean_squared_error(self.df[valcol],fcast[0:len(self.df)])\n",
    "        DF = pd.DataFrame(self.df[valcol])\n",
    "        DF['Type'] = 'Actual'\n",
    "#         DF = DF.reset_index()\n",
    "        DF_fcast = pd.DataFrame(fcast)\n",
    "        DF_fcast['Type'] = 'Predicted'\n",
    "        DF_fcast = DF_fcast.rename(columns={'predicted_mean':valcol})\n",
    "        final_DF = pd.concat([DF,DF_fcast])\n",
    "        final_DF = final_DF.reset_index()\n",
    "#         DF_fcast = DF_fcast.rename(columns={'predicted_mean':'Predicted', 'index':'DateTime'})\n",
    "        DF_val = pd.DataFrame(c1.fit_model(valcol))\n",
    "        DF_val = DF_val.reset_index()\n",
    "        DF_val = DF_val.rename(columns={'index':'Date','predicted_mean':'Validation'})\n",
    "        final_DF = final_DF.rename(columns={'index':'Date'})\n",
    "        print(DF_val)\n",
    "        print(final_DF)\n",
    "#         final_DF =  final_DF.merge(DF_val, on='DateTime',how='outer')\n",
    "        final_DF =  final_DF.merge(DF_val, on='Date',how='outer')\n",
    "        final_DF['Date'] = final_DF['Date'].astype('str')\n",
    "        print(final_DF)\n",
    "        print(final_DF.tail(50))\n",
    "        \n",
    "c1 = TSA(df)\n",
    "c1.adf_test(\"TREATED WATER PRODUCTION IN ML\")\n",
    "c1.determine_ARIMA_order(\"TREATED WATER PRODUCTION IN ML\")\n",
    "c1.fit_model(\"TREATED WATER PRODUCTION IN ML\")\n",
    "c1.full_data_model(\"TREATED WATER PRODUCTION IN ML\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import itertools  # Import itertools for combinations\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "from statsmodels.tools.eval_measures import aic\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_percentage_error\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "\n",
    "def fit_sarima_models(valcol, seasonal_periods):\n",
    "        best_aic = float(\"inf\")\n",
    "        best_order = None\n",
    "        best_seasonal_order = None\n",
    "        best_model = None\n",
    "        \n",
    "        # Define range of p, d, q\n",
    "        p = d = q = range(0, 2)  # Adjust range as needed\n",
    "        \n",
    "        # Loop through all combinations of (p, d, q) and (P, D, Q)\n",
    "        for param in [(x[0], x[1], x[2]) for x in itertools.product(p, d, q)]:\n",
    "            for seasonal_param in [(x[0], x[1], x[2], seasonal_periods) for x in itertools.product(p, d, q)]:\n",
    "                try:\n",
    "                    model = SARIMAX(df[valcol],\n",
    "                                    order=param,\n",
    "                                    seasonal_order=seasonal_param)\n",
    "                    results = model.fit(disp=False)\n",
    "                    \n",
    "                    if results.aic < best_aic:\n",
    "                        best_aic = results.aic\n",
    "                        best_order = param\n",
    "                        best_seasonal_order = seasonal_param\n",
    "                        best_model = results\n",
    "                        \n",
    "                except Exception as e:\n",
    "                    continue\n",
    "        \n",
    "        print(f'Best SARIMA Order: {best_order}')\n",
    "        print(f'Best Seasonal Order: {best_seasonal_order}')\n",
    "        print(f'Best AIC: {best_aic}')\n",
    "        return best_model\n",
    "    \n",
    "fit_sarima_models(\"TREATED WATER PRODUCTION IN ML\",4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "ts_data=df['TREATED WATER PRODUCTION IN ML']\n",
    "# Example of splitting data into different training sets\n",
    "train_data = ts_data[:'2023-12-31']  # Extend training data\n",
    "validation_data = ts_data['2024-01-01':'2024-05-05']\n",
    "test_data = ts_data['2024-05-06':]\n",
    "\n",
    "# Fit the model on the extended training data\n",
    "model = SARIMAX(train_data, order=(0, 1, 1),sarima_order=(0,0,0,52))\n",
    "results = model.fit()\n",
    "\n",
    "# Forecast on validation set\n",
    "forecast = results.get_forecast(steps=len(validation_data))\n",
    "forecast_index = validation_data.index\n",
    "\n",
    "# Calculate forecast accuracy\n",
    "mae = mean_absolute_error(validation_data, forecast.predicted_mean)\n",
    "print(f'Mean Absolute Error: {mae}')\n",
    "\n",
    "# Plot forecast against actual values\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(train_data, label='Training Data')\n",
    "plt.plot(validation_data, label='Validation Data', color='orange')\n",
    "plt.plot(forecast_index, forecast.predicted_mean, label='Forecast', color='green')\n",
    "plt.fill_between(forecast_index, \n",
    "                 forecast.conf_int().iloc[:, 0], \n",
    "                 forecast.conf_int().iloc[:, 1], \n",
    "                 color='green', alpha=0.3)\n",
    "plt.title('Forecast vs Actuals')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "\n",
    "# Fit the model on the training data without seasonality\n",
    "model_no_seasonality = SARIMAX(train_data, order=(0, 1, 1), seasonal_order=(0, 0, 0, 52))\n",
    "results_no_seasonality = model_no_seasonality.fit()\n",
    "\n",
    "# Forecasting\n",
    "forecast_no_seasonality = results_no_seasonality.get_forecast(steps=len(validation_data))\n",
    "forecast_index_no_seasonality = validation_data.index\n",
    "\n",
    "# Plot forecast against actual values\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(train_data, label='Training Data')\n",
    "plt.plot(validation_data, label='Validation Data', color='orange')\n",
    "plt.plot(forecast_index_no_seasonality, forecast_no_seasonality.predicted_mean, label='Forecast', color='green')\n",
    "plt.fill_between(forecast_index_no_seasonality, \n",
    "                 forecast_no_seasonality.conf_int().iloc[:, 0], \n",
    "                 forecast_no_seasonality.conf_int().iloc[:, 1], \n",
    "                 color='green', alpha=0.3)\n",
    "plt.title('Forecast vs Actuals without Seasonal Component')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Fit the ARIMA model on the training data with a trend\n",
    "model_trend = ARIMA(train_data, order=(0, 1, 1))  # Example order (p=1, d=1, q=1)\n",
    "results_trend = model_trend.fit()\n",
    "\n",
    "# Forecasting\n",
    "forecast_trend = results_trend.get_forecast(steps=len(validation_data))\n",
    "forecast_index_trend = validation_data.index\n",
    "\n",
    "# Plot forecast against actual values\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(train_data, label='Training Data')\n",
    "plt.plot(validation_data, label='Validation Data', color='orange')\n",
    "plt.plot(forecast_index_trend, forecast_trend.predicted_mean, label='Forecast', color='green')\n",
    "plt.fill_between(forecast_index_trend, \n",
    "                 forecast_trend.conf_int().iloc[:, 0], \n",
    "                 forecast_trend.conf_int().iloc[:, 1], \n",
    "                 color='green', alpha=0.3)\n",
    "plt.title('Forecast vs Actuals with Trend Component')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.stattools import adfuller\n",
    "from statsmodels.tsa.holtwinters import ExponentialSmoothing\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_percentage_error\n",
    "from statsmodels.tools.eval_measures import rmse\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class TSA:\n",
    "    def __init__(self, df):\n",
    "        self.df = df\n",
    "        \n",
    "    def adf_test(self, valcol):\n",
    "        result = adfuller(self.df[valcol].dropna(), autolag='AIC')\n",
    "    \n",
    "        labels = ['ADF test statistic', 'p-value', '# lags used', '# observations']\n",
    "        out = pd.Series(result[0:4], index=labels)\n",
    "\n",
    "        for key, val in result[4].items():\n",
    "            out[f'critical value ({key})'] = val\n",
    "        \n",
    "        print(out.to_string())\n",
    "    \n",
    "        if result[1] <= 0.05:\n",
    "            print(\"Strong evidence against the null hypothesis. Reject the null hypothesis. Data is stationary.\")\n",
    "            state = \"Stationary\"\n",
    "        else:\n",
    "            print(\"Weak evidence against the null hypothesis. Fail to reject the null hypothesis. Data is non-stationary.\")\n",
    "            state = \"Non-stationary\"\n",
    "        return \"Yes\" if state == \"Stationary\" else \"No\"\n",
    "    \n",
    "    def fit_exponential_smoothing(self, valcol):\n",
    "        train = self.df[self.df.index < '2024-01-07'][valcol]\n",
    "        test = self.df[(self.df.index >= '2024-01-07') & (self.df.index <= '2024-05-05')][valcol]\n",
    "        val = self.df[self.df.index > '2024-05-05'][valcol]\n",
    "\n",
    "        # Fit the model\n",
    "#         model = ExponentialSmoothing(train, trend='add', seasonal='add', seasonal_periods=12).fit()\n",
    "        model = ExponentialSmoothing(train, trend='mul', seasonal=None).fit()\n",
    "        predictions = model.forecast(len(test))\n",
    "\n",
    "        # Calculate error metrics\n",
    "        error1 = mean_squared_error(test, predictions)\n",
    "        error2 = rmse(test, predictions)\n",
    "        error3 = mean_absolute_percentage_error(test, predictions)\n",
    "        accuracy = (1 - error3) * 100\n",
    "\n",
    "        print(f'MSE Error: {error1:.10f}')\n",
    "        print(f'RMSE Error: {error2:.10f}')\n",
    "        print(f'MAPE Error: {error3:.10f}')\n",
    "        print(f'Accuracy: {accuracy:.10f}')\n",
    "\n",
    "        model_results = {\n",
    "            \"Model\": \"Exponential Smoothing\",\n",
    "            \"Stationary\": self.adf_test(valcol),\n",
    "            \"X_train\": str(len(train)) + \" Weeks\",\n",
    "            \"X_test\": str(len(test)) + \" Weeks\",\n",
    "            \"X_validation\": str(len(val)) + \" Weeks\",\n",
    "            \"MSE\": error1,\n",
    "            \"RMSE\": error2,\n",
    "            \"MAPE\": error3,\n",
    "            \"Accuracy\": round(accuracy, 0)\n",
    "        }\n",
    "        return predictions\n",
    "\n",
    "    def full_data_model(self, valcol):\n",
    "#         model = ExponentialSmoothing(self.df[valcol], trend='add', seasonal='add', seasonal_periods=12).fit()\n",
    "        model = ExponentialSmoothing(self.df[valcol], trend='mul', seasonal=None).fit()\n",
    "        fcast = model.forecast(4).round(2)  # Forecast for next 3 periods\n",
    "\n",
    "        DF = pd.DataFrame(self.df[valcol])\n",
    "        DF['Type'] = 'Actual'\n",
    "        DF_fcast = pd.DataFrame(fcast, columns=[valcol])\n",
    "        DF_fcast['Type'] = 'Predicted'\n",
    "        \n",
    "        final_DF = pd.concat([DF, DF_fcast])\n",
    "        final_DF = final_DF.reset_index()\n",
    "        \n",
    "        predictions_val = self.fit_exponential_smoothing(valcol)\n",
    "        DF_val = pd.DataFrame(predictions_val, columns=['Validation'])\n",
    "        final_DF = final_DF.merge(DF_val, left_on='index', right_index=True, how='outer')\n",
    "        final_DF['Date'] = pd.to_datetime(final_DF['index'])\n",
    "        \n",
    "        \n",
    "\n",
    "        print(final_DF)\n",
    "        print(final_DF.tail(50))\n",
    "        \n",
    "        \n",
    "        # Plotting the actual and predicted values\n",
    "        plt.figure(figsize=(12, 4))\n",
    "        plt.plot(self.df.index, self.df[valcol], label='Actual', color='blue', marker='o')\n",
    "        plt.plot(final_DF['Date'], final_DF[valcol], label='Predicted', color='orange', linestyle='--', marker='x')\n",
    "        plt.title('Actual vs Predicted Water Production')\n",
    "        plt.xlabel('Date')\n",
    "        plt.ylabel('Treated Water Production (ML)')\n",
    "        plt.legend()\n",
    "        plt.xticks(rotation=45)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "\n",
    "c1 = TSA(df)\n",
    "c1.adf_test(\"TREATED WATER PRODUCTION IN ML\")\n",
    "c1.fit_exponential_smoothing(\"TREATED WATER PRODUCTION IN ML\")\n",
    "c1.full_data_model(\"TREATED WATER PRODUCTION IN ML\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
